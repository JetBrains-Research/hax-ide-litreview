[
    {
        "Paper":"\u201cIt would work for me too\u201d: How Online Communities Shape Software Developers\u2019 Trust in AI-Powered Code Generation Tools",
        "Authors":[
            "Ruijia Cheng",
            "Ruotong Wang",
            "Thomas Zimmermann",
            "Denae Ford"
        ],
        "Affiliation":[
            "Academia",
            "Industry"
        ],
        "DOI":"https:\/\/doi.org\/10.1145\/3651990",
        "Date":2024,
        "Venue":[
            "ACM Transactions on Interactive Intelligent Systems"
        ],
        "Goal":[
            "The goal of the study is to investigate how online communities shape software developers' trust in AI-powered code generation tools."
        ],
        "Research Questions":[
            "1. How do online communities shape software developers' trust in AI-powered code generation tools?\n2. What are the design opportunities of incorporating a user community into the experience of AI code generation tools to help developers form appropriate trust?"
        ],
        "Key Findings":[
            "1) Online communities shape developers' trust in AI-powered code generation tools through two pathways: collective sensemaking and community heuristics.\n2) Developers face several challenges in engaging with community-curated experiences and evaluation signals",
            "including lack of dedicated channels",
            "high cost of sharing",
            "and missing details for reproducibility.\n3) The study provides design recommendations for integrating user communities into AI systems to help build appropriate trust",
            "such as scaffolding users to share experiences",
            "presenting content hierarchically",
            "and enabling users to apply community insights to their use cases."
        ],
        "Future Work":[
            "1) Exploring the role of communities in building trust for other generative AI applications beyond just code generation tools\n2) Investigating the longitudinal impact of communities on trust building as AI code generation tools and user experiences evolve over time\n3) Examining the role of communities and trust for the \"next generation\" of software developers who may be enabled by AI to create software",
            "beyond just current professional developers"
        ],
        "Scope from our taxonomy":[
            "Impact"
        ],
        "SDLC stage from waterfall":[
            "Unspecified"
        ],
        "Context":[
            "Professional"
        ],
        "Eligibility score":[

        ],
        "ID":"0"
    },
    {
        "Paper":"\u201cIt\u2019s Weird That it Knows What I Want\u201d: Usability and Interactions with Copilot for Novice Programmers",
        "Authors":[
            "James Prather",
            "Brent N Reeves",
            "Paul Denny",
            "Brett A Becker",
            "Andrew Luxton-Reilly",
            "Garrett Powell",
            "James Finnie-Ansley",
            "Juho Leinonen",
            "James Finnie- Ansley",
            "Eddie Antonio Santos"
        ],
        "Affiliation":[
            "Academia"
        ],
        "DOI":"https:\/\/doi.org\/10.1145\/3617367",
        "Date":2023,
        "Venue":[
            "ACM Transactions on Computer-Human Interaction"
        ],
        "Goal":[
            "The goal of this paper is to explore how novice programmers interact with and perceive the use of the AI code generation tool GitHub Copilot",
            "including observing their interactions",
            "understanding their perceptions of the benefits and dangers",
            "and contributing new interaction patterns and design implications."
        ],
        "Research Questions":[
            "RQ1: How do novices interact with GitHub Copilot when they first encounter it?\nRQ2: How do novices perceive their first-time experience of using GitHub Copilot?"
        ],
        "Key Findings":[
            "1. Novice programmers struggle to understand and use Copilot effectively",
            "often drifting aimlessly between Copilot's suggestions without making progress.\n2. Novices expressed both positive and negative sentiments about using Copilot",
            "with excitement about its capabilities but also frustration when the suggestions were unhelpful or incorrect.\n3. There are several negative interaction patterns that novices demonstrate when using Copilot like Shepherding (type out suggestions character by character) or Drifting (adapting incorrect code which leads further away from a solution)\n4. Novices noticed the benefits in learning (exploration) and going faster (acceleration) with Copilot.\n5. Novices were concerned about the ethical implications of using Copilot",
            "such as over-reliance",
            "academic misconduct",
            "and bias in the training data."
        ],
        "Future Work":[
            "1) Studying how students' use and interaction with Copilot evolves over a longer period of time",
            "such as an entire semester",
            "as they gain more programming experience.\n2) Investigating the reasons behind the observations made in the current study",
            "such as whether the constant presence of Copilot's code suggestions increases the cognitive load for novice programmers.\n3) Examining how AI code generators like Copilot can be most effectively incorporated into introductory programming classrooms.\n4) Investigating how such design implications as control of the prompt length",
            "metacognitive scaffolding (providing an unobtrusive UI element directly above the suggested code to allow users to cycle through different code suggestions)",
            "utilizing user-centered XAI design techniques",
            "e.g.",
            "by presenting conversational dialogues."
        ],
        "Scope from our taxonomy":[
            "Impact"
        ],
        "SDLC stage from waterfall":[
            "Unspecified"
        ],
        "Context":[
            "Educational"
        ],
        "Eligibility score":[

        ],
        "ID":"1"
    },
    {
        "Paper":"A Case Study in Engineering a Conversational Programming Assistant's Persona",
        "Authors":[
            "Steven I Ross",
            "Michael Muller",
            "Fernando Martinez",
            "Ibm Argentina",
            "Argentina Stephanie Houde",
            "Justin D Weisz",
            "Stephanie Houde"
        ],
        "Affiliation":[
            "Industry"
        ],
        "DOI":"https:\/\/doi.org\/10.48550\/arXiv.2301.10016",
        "Date":2023,
        "Venue":[
            "HAI-GEN at IUI'2023"
        ],
        "Goal":[
            "The goal of this paper is to present a case study on how the authors engineered the prompt for a conversational programming assistant called the \"Programmer's Assistant\" in order to achieve the desired persona and behavior."
        ],
        "Research Questions":[
            "1. Could the general-purpose large language model power a conversational programming assistant?\n3. Whether a conversational interaction style would prove useful and acceptable to potential users?"
        ],
        "Key Findings":[
            "1. Prompt Engineering: Iterative prompt modifications helped establish a conversational and helpful assistant persona.\n2. Reducing Overconfidence: Emphasizing humility in responses reduced overconfidence and made the assistant more receptive to corrections.\n3. Artifact-Centric Interaction: Enabling context-aware interactions improved user experience",
            "especially with code-related queries."
        ],
        "Future Work":[
            "1. Using a dynamic prompt with more relevant examples or retrieving pertinent information to improve the assistant's responses.\n2. Improving the memory management of the assistant by removing redundant information while preserving useful context.\n3. Exploring \"internal deliberation\" to improve the reasoning and explanations provided by the assistant."
        ],
        "Scope from our taxonomy":[
            "Form"
        ],
        "SDLC stage from waterfall":[
            "Unspecified"
        ],
        "Context":[
            "Professional"
        ],
        "Eligibility score":[

        ],
        "ID":"2"
    },
    {
        "Paper":"A Case Study on Scaffolding Exploratory Data Analysis for AI Pair Programmers",
        "Authors":[
            "Haoquan Zhou",
            "Jingbo Li"
        ],
        "Affiliation":[
            "Academia"
        ],
        "DOI":"https:\/\/doi.org\/10.1145\/3544549.3583943",
        "Date":2023,
        "Venue":[
            "CHI EA'23"
        ],
        "Goal":[
            "The goal of the study is to investigate how data scientists can use the AI-based code generation tool GitHub Copilot to aid in exploratory data analysis tasks",
            "and to understand the factors that influence the performance of GitHub Copilot in these tasks",
            "as well as the potential drawbacks and future improvements needed for GitHub Copilot to be more useful for data science."
        ],
        "Research Questions":[
            "1) How do the number of prompts",
            "the structure of prompts",
            "and the interaction pattern between data scientists and GitHub Copilot",
            "infuence the performance of GitHub Copilot in exploratory data analysis tasks?\n2) What are the potential drawbacks of using GitHub Copilot?\n3) What future improvements can be made to GitHub Copilot for use in data science?"
        ],
        "Key Findings":[
            "1. The choice of terminology",
            "arrangement of word order",
            "and repetition of keywords in the prompts can influence whether GitHub Copilot can generate functionally correct code.\n2. Insufficient interaction between the human and GitHub Copilot can lead to either over-trusting or under-trusting the AI's abilities",
            "resulting in the failure of data analysis tasks.\n3. GitHub Copilot has limitations",
            "such as ignoring information in the prompts provided by users."
        ],
        "Future Work":[
            "1. Expand the sample size to get more robust and generalizable findings.\n2. Investigate the stability and consistency issues with GitHub Copilot's code generation.\n3. Propose feasible solutions to the problems identified",
            "such as ways to improve the interaction between users and GitHub Copilot."
        ],
        "Scope from our taxonomy":[
            "Impact"
        ],
        "SDLC stage from waterfall":[
            "Coding: In this phase the whole requirements will be converted to the production environment."
        ],
        "Context":[
            "Professional"
        ],
        "Eligibility score":[

        ],
        "ID":"3"
    },
    {
        "Paper":"A Large-Scale Survey on the Usability of AI Programming Assistants: Successes and Challenges",
        "Authors":[
            "Jenny T Liang",
            "Chenyang Yang",
            "Brad A Myers"
        ],
        "Affiliation":[
            "Academia"
        ],
        "DOI":"https:\/\/doi.org\/10.48550\/arXiv.2303.17125",
        "Date":2023,
        "Venue":[
            "International Conference on Software Engineering"
        ],
        "Goal":[
            "The goal of this paper is to understand developers' practices while using AI programming assistants and the important usability challenges they face."
        ],
        "Research Questions":[
            "1. What are the usage practices and motivations of developers when using AI programming assistants?\n2. What are the key usability challenges that developers face when using AI programming assistants?"
        ],
        "Key Findings":[
            "1. Participants who were GitHub Copilot users reported a median of 30.5% of their code being written with its help.\n2. The most important reasons for using AI programming assistants were for autocomplete",
            "completing programming tasks faster",
            "or skipping going online to recall syntax.\n3. Participants successfully used these tools to generate code that was repetitive or had simple logic",
            "but the most important reasons for not using AI programming assistants were because the code that the tools generated did not meet functional or non-functional requirements and because it was difficult to control the tool.\n4. The most frequent usability challenges participants reported encountering were understanding what part of the input caused the outputted code",
            "giving up on using the outputted code",
            "and controlling the tool to generate helpful code suggestions.\n5. Participants most often gave up on outputted code because the code did not perform the intended action or did not account for certain fun"
        ],
        "Future Work":[
            "1. Investigating new interaction techniques to support the \"acceleration mode\" usage of AI programming assistants",
            "where developers use the tools to quickly complete known tasks",
            "while requiring minimal cognitive effort to avoid distracting the developers.\n2. Understand what can be done to support the \"exploration mode\" usage of AI\n3. Studying how to better align AI programming assistants with the needs and requirements of developers",
            "including facilitating ways for developers to explicitly describe their software design knowledge to the tools.\n4. Investigating methods for supporting chat-based interactions with AI programming assistants",
            "similar to ChatGPT",
            "without negatively impacting developers' efficiency and flow while programming.\n5. Get more data on the people not interested in AI who are underrepresented in the current study\n6. Design new metrics to increase developer-tool alignment\n7. Investigate avenues for incorporating non-functional requirements such as readability and per"
        ],
        "Scope from our taxonomy":[
            "Impact",
            "Quality"
        ],
        "SDLC stage from waterfall":[
            "Unspecified"
        ],
        "Context":[
            "Professional"
        ],
        "Eligibility score":[

        ],
        "ID":"4"
    },
    {
        "Paper":"A Mixed Reality Approach for Innovative Pair Programming Education with a Conversational AI Virtual Avatar",
        "Authors":[
            "Gilda Manfredi",
            "Ugo Erra",
            "and Gabriele Gilio"
        ],
        "Affiliation":[
            "Academia"
        ],
        "DOI":"https:\/\/doi.org\/10.1145\/3593434.3593952",
        "Date":2023,
        "Venue":[
            "EASE'23"
        ],
        "Goal":[
            "The goal of this paper is to present a novel approach that utilizes mixed reality (MR) technology and a conversational intelligent virtual avatar to enable users to learn the pair programming (PP) methodology without the need for a human partner",
            "addressing the challenges of distributed and in-person PP."
        ],
        "Research Questions":[
            "Can using the Virtual Avatar tool increase productivity and coding skills?"
        ],
        "Key Findings":[
            "1. The proposed MR educational solution for pair programming (PP) leverages a conversational intelligent virtual avatar to guide and provide suggestions to the user during the PP process.\n2. The system integrates a conversational agent (CA) extension into Visual Studio Code to provide real-time suggestions for code improvement",
            "which are then transmitted to the HoloLens MR application.\n3. The virtual avatar speaks and interacts with the user",
            "creating an immersive learning experience",
            "and a single developer can learn and apply the PP methodology without a human partner in the MR environment.\n4. Preliminary results indicate that participants who used the proposed application for PP showed a statistically significant improvement in their coding skills compared to the control group",
            "and they expressed high satisfaction with the application's features",
            "usability",
            "and effectiveness in addressing the challenges of finding and collaborating with a human partner."
        ],
        "Future Work":[
            "The future work suggested in the paper is to conduct more extensive and controlled evaluations of the MR educational solution for pair programming",
            "in order to obtain more robust evidence of its effectiveness. This includes investigating the long-term impact of the application on participants' learning outcomes and refining the application based on the feedback received."
        ],
        "Scope from our taxonomy":[
            "Form"
        ],
        "SDLC stage from waterfall":[
            "Unspecified"
        ],
        "Context":[
            "Educational"
        ],
        "Eligibility score":[

        ],
        "ID":"5"
    },
    {
        "Paper":"A Study on Developer Behaviors for Validating and Repairing LLM-Generated Code Using Eye Tracking and IDE Actions",
        "Authors":[
            "Ningzhi Tang",
            "Meng Chen",
            "Zheng Ning",
            "Aakash Bansal",
            "Yu Huang",
            "Collin Mcmillan",
            "Toby Jia",
            "Jun Li"
        ],
        "Affiliation":[
            "Academia"
        ],
        "DOI":"DOI: 10.1109\/VL\/HCC60511.2024.00015",
        "Date":2024,
        "Venue":[
            "VL\/HCC'24"
        ],
        "Goal":[
            "The goal of this paper is to investigate how developers validate and repair code generated by large language models (LLMs)",
            "specifically GitHub Copilot",
            "and to examine the impact of code provenance awareness on their validation and repair strategies."
        ],
        "Research Questions":[
            "RQ1. What is developers\u2019 perception of LLM-generated code compared to human-written code?\nRQ2. What are the differences in the strategies of developers to validate and repair LLM-generated code versus human-written code? \nRQ3. How does awareness of code provenance affect the validation and repair behaviors of developers?"
        ],
        "Key Findings":[
            "1. Developers often do not recognize the provenance of code unless explicitly informed.\n2. LLM-generated code exhibits better coding style and readability compared to human-written code",
            "but also unique errors that human developers do not typically make.\n3. Developers employ similar validation and repair strategies for LLM-generated code as they do for human-written code",
            "but also display distinct behaviors like frequent switching between code and comments and a tendency to delete and rewrite the code."
        ],
        "Future Work":[
            "1. Systematically investigating the characteristics of LLM-generated code from a human perception perspective",
            "to inform the development of more effective tools for developers working with LLM-generated code.\n2. Expanding the research to include professional developers in long-term",
            "real-world studies.\n3. Incorporating additional biometric sensors",
            "such as heart rate monitors and fMRI",
            "to gain a better understanding of the cognitive processes involved in software development."
        ],
        "Scope from our taxonomy":[
            "Impact"
        ],
        "SDLC stage from waterfall":[
            "Maintenance: After the software is already released",
            "it may need some modifications",
            "improvements",
            "errors correction",
            "and refinement accordingly."
        ],
        "Context":[
            "Professional"
        ],
        "Eligibility score":[

        ],
        "ID":"6"
    },
    {
        "Paper":"A Transformer-Based Approach for Smart Invocation of Automatic Code Completion",
        "Authors":[
            "Aral de Moor",
            "Arie van Deursen",
            "Maliheh Izadi"
        ],
        "Affiliation":[
            "Academia"
        ],
        "DOI":"https:\/\/doi.org\/10.1145\/3664646.3664760",
        "Date":2024,
        "Venue":[
            "AIware'24"
        ],
        "Goal":[
            "The goal of the paper is to train a machine learning model that can accurately predict when to invoke a code completion tool given the code context and available telemetry data."
        ],
        "Research Questions":[
            "1. How to proactively predict when to invoke a code completion model based on code context and telemetry data?"
        ],
        "Key Findings":[
            "1. Transformer-based models like CodeBERTa can significantly outperform baseline logistic regression models at filtering out unhelpful code completion suggestions",
            "by leveraging the semantic understanding of code context.\n2. Integrating telemetry data (user interaction features) with the code context in a transformer model",
            "like the JonBERTa variants",
            "can further improve the accuracy of the invocation filtering.\n3. The proposed invocation filtering models were effective in a real-world deployment with 34 developers",
            "with the JonBERTa-head model performing the best in terms of balancing completion quality and timing."
        ],
        "Future Work":[
            "1. Exploring the search space further using a larger dataset to better represent the diverse behaviors of developers.\n2. Investigating the personalization of the invocation-filtering system",
            "as related work has shown promising results in this area.\n3. Studying the long-term impact of the code completions on developer productivity and code quality",
            "as delivering completions exactly when requested may not always align with developers' true needs."
        ],
        "Scope from our taxonomy":[
            "Impact",
            "Quality"
        ],
        "SDLC stage from waterfall":[
            "Coding: In this phase the whole requirements will be converted to the production environment."
        ],
        "Context":[
            "Professional"
        ],
        "Eligibility score":[

        ],
        "ID":"7"
    },
    {
        "Paper":"A User-centered Security Evaluation of Copilot",
        "Authors":[
            "Owura Asare",
            "N Asokan",
            "Meiyappan Nagappan"
        ],
        "Affiliation":[
            "Academia"
        ],
        "DOI":"https:\/\/doi.org\/10.1145\/3597503.3639154",
        "Date":2024,
        "Venue":[
            "ICSE'24"
        ],
        "Goal":[
            "This paper aims to perform a user-centered evaluation of GitHub's Copilot to better understand its strengths and weaknesses regarding code security."
        ],
        "Research Questions":[
            "(1) Does Copilot use correlate with participants writing more secure code?\n(2) Are there vulnerability types that Copilot is more susceptible to or more resilient against?"
        ],
        "Key Findings":[
            "1. The use of Copilot appears to correlate with participants writing more secure code",
            "but only for the more difficult problem. For the easier problem",
            "there was no significant difference in security performance between the group that used Copilot and the group that did not.\n2. The use of Copilot did not have a disproportionate impact on the presence of any particular vulnerability type."
        ],
        "Future Work":[
            "1. Conducting a more targeted user study with multiple problems of varying difficulty levels",
            "and using proxies for both problem difficulty and security to further investigate the finding that Copilot may be more beneficial for harder problems.\n2. Ensuring that future research and testing of code generation tools use problems that are above a certain level of complexity rather than trivial problems to get more meaningful and generalizable results.\n3. Exploring how the security experience of users of code generation tools affects the security of the code they generate with the assistance of these tools."
        ],
        "Scope from our taxonomy":[
            "Quality"
        ],
        "SDLC stage from waterfall":[
            "Coding: In this phase the whole requirements will be converted to the production environment."
        ],
        "Context":[
            "Professional"
        ],
        "Eligibility score":[

        ],
        "ID":"8"
    },
    {
        "Paper":"AI Tool Use and Adoption in Software Development by Individuals and Organizations: A Grounded Theory Study",
        "Authors":[
            "Z E Shi",
            "Li",
            "Daniela Damian",
            "Bowen Xu",
            "Ze Shi Li",
            "Nowshin Nawar Arony",
            "Ahmed Musa Awon"
        ],
        "Affiliation":[
            "Academia"
        ],
        "DOI":"https:\/\/doi.org\/10.48550\/arXiv.2406.17325",
        "Date":2024,
        "Venue":[
            "arXiv"
        ],
        "Goal":[
            "The goal of this publication is to fill a research gap in understanding the factors that impact the adoption and use of AI tools by individuals and organizations in software development."
        ],
        "Research Questions":[
            "1) What are the individual and organizational motives that impact the adoption and use of AI tools in software development?\n2) What are the individual and organizational challenges that impact the adoption and use of AI tools in software development?"
        ],
        "Key Findings":[
            "1. Individual Motives to use AI tools: Learning and Debugging",
            "Discussion with peers\n2. Organizational Motives: Creating a culture of sharing",
            "Providing and promoting the tool",
            "Providing guidance and training\n3. Individual Challenges that limit AI tool use and adoption: Fear of decreased skills",
            "Limited AI Capabilities for Software Development",
            "Lack of prompting skill",
            "s Potential judgment\n4. Organizational Challenges: Lack of culture of sharing",
            "Company does not cover the cost of the tool",
            "Lack of company guideline"
        ],
        "Future Work":[
            "1. Investigating security and privacy mechanisms to protect confidential data when using AI tools",
            "including prompt sanitization.\n2. Conducting more empirical research on the organizational factors",
            "such as culture and size",
            "that impact the use and adoption of AI tools by practitioners.\n3. Studying the content and structure of training materials and manuals to help practitioners become more effective users of AI tools",
            "particularly around prompt engineering."
        ],
        "Scope from our taxonomy":[
            "Impact"
        ],
        "SDLC stage from waterfall":[
            "Unspecified"
        ],
        "Context":[
            "Professional"
        ],
        "Eligibility score":[

        ],
        "ID":"9"
    },
    {
        "Paper":"AI-Powered Chatbots and the Transformation of Work: Findings from a Case Study in Software Development and Software Engineering",
        "Authors":[
            "Thomas S\u00fc\u00dfe",
            "Maria Kobert",
            "Simon Grapenthin",
            "Bernd-Friedrich Voigt"
        ],
        "Affiliation":[
            "Academia"
        ],
        "DOI":"https:\/\/doi.org\/10.1007\/978-3-031-42622-3_49",
        "Date":2023,
        "Venue":[
            "PRO-VE 2023"
        ],
        "Goal":[
            "To explore the transformative effects of AI-powered chatbots",
            "like ChatGPT and GitHub Copilot",
            "in software development by analyzing how employees adapt to these tools and the emerging AI-related coping patterns necessary for effective collaboration."
        ],
        "Research Questions":[
            "1. How do AI-powered chatbots impact software development workflows?\n2. What cognitive",
            "emotional",
            "and social coping patterns are necessary for successful AI-human collaboration?\n3. What role do AI-related competencies play in these interactions?"
        ],
        "Key Findings":[
            "1. Identified 14 distinct coping patterns categorized into cognitive",
            "emotional",
            "and social dimensions.\n2. Developers benefit from understanding how AI functions",
            "including task division and reflecting on AI outputs.\n3. Emotional adaptations include viewing AI as a colleague and being open to technological change.\n4. Social adjustments emphasize ethical standards and effective communication with AI agents."
        ],
        "Future Work":[
            "1. Broaden empirical studies across diverse industries and roles to validate findings.\n2. Further refine the AI competency framework to guide AI implementation in various work settings.\n3. Develop interventions to support employees\u2019 skill-building in AI competencies for sustainable human-AI collaboration."
        ],
        "Scope from our taxonomy":[
            "Impact"
        ],
        "SDLC stage from waterfall":[
            "Unspecified"
        ],
        "Context":[
            "Professional"
        ],
        "Eligibility score":[

        ],
        "ID":"10"
    },
    {
        "Paper":"An Analysis of the Costs and Benefits of Autocomplete in IDEs",
        "Authors":[
            "Shaokang Jiang",
            "Michael Coblenz"
        ],
        "Affiliation":[
            "Academia"
        ],
        "DOI":"https:\/\/doi.org\/10.1145\/3580435",
        "Date":2024,
        "Venue":[
            "Proceedings of the ACM on Software Engineering"
        ],
        "Goal":[
            "The goal of this paper is to evaluate the costs and benefits of autocomplete in IDEs",
            "particularly in terms of programmer productivity",
            "learning",
            "and documentation usage."
        ],
        "Research Questions":[
            "RQ1: Does autocomplete affect how productive developers are?\nRQ2: How does autocomplete usage affect how much developers learn about APIs?\nRQ3: How does autocomplete usage affect the time spent reading the documentation?\nRQ4: How does autocomplete usage affect the number of keystrokes programmers enter?"
        ],
        "Key Findings":[
            "1. Autocomplete significantly reduced the time participants spent reading documentation",
            "but did not significantly reduce the total number of keystrokes required to complete the tasks.\n2. Autocomplete significantly improved participants' learning of the API",
            "as measured by their quiz scores.\n3. The benefits of autocomplete were comparable to 7-15 years of programming experience."
        ],
        "Future Work":[
            "1) Investigating how the results apply to other programming languages beyond Java\n2) Studying the benefits of autocomplete in longer-term",
            "longitudinal settings beyond the limited scope of this study\n3) Comparing the benefits of autocomplete for professional programmers versus students\n4) Examining the implications of autocomplete for APIs that are familiar versus unfamiliar to users\n5) Utilizing more advanced eye tracking capabilities to collect richer data and insights",
            "such as by analyzing fixations rather than just raw gaze data"
        ],
        "Scope from our taxonomy":[
            "Impact"
        ],
        "SDLC stage from waterfall":[
            "Coding: In this phase the whole requirements will be converted to the production environment."
        ],
        "Context":[
            "Professional"
        ],
        "Eligibility score":[

        ],
        "ID":"11"
    },
    {
        "Paper":"An Empirical Evaluation of GitHub Copilot's Code Suggestions",
        "Authors":[
            "Nhan Nguyen",
            "Sarah Nadi"
        ],
        "Affiliation":[
            "Academia"
        ],
        "DOI":"https:\/\/doi.org\/10.1145\/3524842.3528470",
        "Date":2022,
        "Venue":[
            "MSR'22"
        ],
        "Goal":[
            "The goal of this paper is to empirically evaluate the correctness and understandability of the code suggestions generated by GitHub Copilot."
        ],
        "Research Questions":[
            "RQ1: How correct are Copilot\u2019s code suggestions?\nRQ2: How understandable is the code provided by Copilot?"
        ],
        "Key Findings":[
            "1. Copilot\u2019s correctness (passing all tests) varies by language",
            "with Java as highest (57%) and JavaScript lowest (27%).\n2. The median cognitive complexity and cyclomatic complexity of Copilot solutions is 6 and 5",
            "respectively",
            "with no statistically significant differences between languages."
        ],
        "Future Work":[
            "1. Conducting a larger-scale study with more queries to better evaluate Copilot's capabilities.\n2. Gaining more insight into Copilot's internal model and training data to better understand its performance.\n3. Evaluating Copilot's behavior in less ideal situations",
            "such as with less-than-ideal query contexts",
            "and assessing its other capabilities beyond just converting comments to code."
        ],
        "Scope from our taxonomy":[
            "Quality"
        ],
        "SDLC stage from waterfall":[
            "Unspecified"
        ],
        "Context":[
            "Professional"
        ],
        "Eligibility score":[

        ],
        "ID":"12"
    },
    {
        "Paper":"An Empirical Study of Code Search in Intelligent Coding Assistant: Perceptions, Expectations, and Directions",
        "Authors":[
            "Chao Liu",
            "Xindong Zhang",
            "Hongyu Zhang",
            "Zhiyuan Wan",
            "Zhan Huang",
            "Meng Yan"
        ],
        "Affiliation":[
            "Academia",
            "Industry"
        ],
        "DOI":"https:\/\/doi.org\/10.1145\/3663529.3663848",
        "Date":2024,
        "Venue":[
            "FSE Companion'24"
        ],
        "Goal":[
            "The goal of this paper is to conduct a comprehensive empirical investigation into the code search capability of the TONGYI Lingma coding assistant developed by Alibaba Cloud",
            "and to provide insights and future directions for both researchers and practitioners in the area of code search."
        ],
        "Research Questions":[
            "RQ1: Which code search task is frequently used?  \nRQ2: What are the characteristics of queries?  \nRQ3: How many search results are commonly viewed by developers?  \nRQ4: What are developers\u2019 perceptions on the code search in practice?  \nRQ5: What are developers\u2019 purposes of code search?  \nRQ6: What are developers\u2019 expected code search inputs?  \nRQ7: What are developers\u2019 expected extensions on the codebase?  \nRQ8: What are developers\u2019 expected search results?  \nRQ9: Code Search vs. ChatGPT."
        ],
        "Key Findings":[
            "Finding 1: Semantic search has higher usage than API search",
            "with 40% of users using both together. For effective support",
            "code search tools need to consistently yield practical",
            "relevant results.\n\nFinding 2: User queries are generally short",
            "though users may enter longer queries by following tool recommendations or copying runtime logs for debugging. In semantic search",
            "using the local language (e.g.",
            "Chinese) benefits non-English speakers.\n\nFinding 3: Users typically limit their exploration to the first page of search results",
            "with key results often appearing around the second rank.\n\nFinding 4: Experienced developers use code search less frequently but expect it to be highly effective when needed. While semantic search is more commonly used",
            "it can be less effective due to the difficulty of forming precise queries.\n\nFinding 5: Developers rely on code search tools to implement requirements",
            "understand examples",
            "and fix bugs. They expect concise",
            "representative results relevant to thei"
        ],
        "Future Work":[
            "1. Investigate the role of code search in the entire software development lifecycle.\n2. Develop benchmarks and models that address specific search requirements",
            "bridging gaps between academia and industry.\n3. Design targeted test cases for automated validation",
            "similar to the HumanEval benchmark for code generation.\n4. Explore methods to adapt search results more effectively to the intended coding context."
        ],
        "Scope from our taxonomy":[
            "Impact",
            "Other"
        ],
        "SDLC stage from waterfall":[
            "Unspecified"
        ],
        "Context":[
            "Professional"
        ],
        "Eligibility score":[

        ],
        "ID":"13"
    },
    {
        "Paper":"An Empirical Study on Usage and Perceptions of LLMs in a Software Engineering Project",
        "Authors":[
            "Sanka Rasnayaka",
            "Guanlin Wang",
            "Ridwan Shariffdeen",
            "Ganesh Neelakanta Iyer"
        ],
        "Affiliation":[
            "Academia"
        ],
        "DOI":"https:\/\/doi.org\/10.1145\/3643795.3648379",
        "Date":2024,
        "Venue":[
            "LLM4Code'24"
        ],
        "Goal":[
            "The goal of the study is to explore the usefulness of Large Language Models (LLMs) for software development projects in an academic setting."
        ],
        "Research Questions":[
            "1. What are the possible uses and benefits of using LLMs in software engineering projects?\n2. What are the perceptions and attitudes of computer science students towards using LLMs for software development?\n3. What are the specific use cases and benefits of using LLMs in different stages of the software development lifecycle?"
        ],
        "Key Findings":[
            "1. LLMs can play a crucial role in the early stages of software development",
            "especially in generating foundational code structures",
            "and helping with syntax and error debugging.\n2. The adoption of LLMs varied depending on the students' coding skills and prior experience with AI generators",
            "indicating a learning curve in using these tools effectively.\n3. There was no significant difference in the correctness and quality of developed software between teams with heavy and no AI usage",
            "indicating that with proper human intervention",
            "the use of AI tools does not inherently compromise the quality and correctness of code."
        ],
        "Future Work":[
            "1. Developing better training and support mechanisms to help students and software engineers effectively leverage LLMs",
            "especially those with lower coding skills or less experience with AI tools.\n2. Conducting a more comprehensive data collection process to better understand the impact of LLMs on code quality and correctness",
            "as the current study was limited by incomplete data.\n3. Expanding the scope of the study to include a larger and more diverse sample of participants",
            "as well as exploring the use of LLMs in industry settings where additional constraints and considerations may arise."
        ],
        "Scope from our taxonomy":[
            "Impact"
        ],
        "SDLC stage from waterfall":[
            "Unspecified"
        ],
        "Context":[
            "Educational"
        ],
        "Eligibility score":[

        ],
        "ID":"14"
    },
    {
        "Paper":"An Exploratory Study on Upper-Level Computing Students' Use of Large Language Models as Tools in a Semester-Long Project",
        "Authors":[
            "Ben Arie Tanay",
            "Lexy Arinze",
            "Siddhant S Joshi",
            "Kirsten A Davis",
            "James C Davis"
        ],
        "Affiliation":[
            "Academia"
        ],
        "DOI":"DOI 10.18260\/1-2--46557",
        "Date":2024,
        "Venue":[
            "ASEE Annual Conference & Exposition"
        ],
        "Goal":[
            "The goal of this study is to explore how upper-level computing students use large language models (LLMs) as tools in a semester-long software engineering project."
        ],
        "Research Questions":[
            "RQ1: How do students integrate LLMs into coursework when policies allow unrestricted access? \nRQ2: How does the use of LLMs influence students' perceptions of their learning?"
        ],
        "Key Findings":[
            "1. Students used LLMs for a variety of tasks in their software engineering project",
            "including programming support",
            "idea generation",
            "writing support",
            "and project management.\n2. Students perceived both benefits and drawbacks to using LLMs in their learning. With LLMs facilitating self-sufficiency",
            "knowledge acquisition",
            "and solution implementation. But also raising concerns about knowledge retention",
            "prerequisite knowledge",
            "and over-relying on LLMs."
        ],
        "Future Work":[
            "1. Investigating how the use of LLMs influences the learning abilities of students",
            "especially those earlier in the computer engineering curriculum (e.g.",
            "freshman and sophomore level courses).\n2. Exploring how LLMs can be safely integrated into lower-level courses without negatively impacting student learning outcomes.\n3. Studying student interactions with LLMs like ChatGPT as a source of feedback to improve courses and understand how LLM use impacts student work and learning outcomes."
        ],
        "Scope from our taxonomy":[
            "Impact"
        ],
        "SDLC stage from waterfall":[
            "Unspecified"
        ],
        "Context":[
            "Educational"
        ],
        "Eligibility score":[

        ],
        "ID":"15"
    },
    {
        "Paper":"Analyzing Prompt Influence on Automated Method Generation: An Empirical Study with Copilot",
        "Authors":[
            "Ionut Daniel Fagadau",
            "Leonardo Mariani",
            "Daniela Micucci",
            "Oliviero Riganelli",
            "Oliviero Rig"
        ],
        "Affiliation":[
            "Academia"
        ],
        "DOI":"https:\/\/doi.org\/10.1145\/3643916.3644409",
        "Date":2024,
        "Venue":[
            "ICPC'24"
        ],
        "Goal":[
            "The goal of this paper is to systematically investigate the influence of eight prompt features on the style",
            "content",
            "correctness",
            "complexity",
            "size",
            "and similarity to developers' code of the code generated by the Copilot AI assistant."
        ],
        "Research Questions":[
            "RQ1 - How do prompt features impact on the correctness of the code generated by Copilot? \nRQ2 - How do prompt features impact on the complexity of the code generated by Copilot? \nRQ3 - How do prompt features impact on the similarity between the code generated by Copilot and the code implemented by the developers?"
        ],
        "Key Findings":[
            "1. Including a summary of the purpose of the method and including examples in the prompt are particularly useful to obtain code that passes the available test cases.\n2. Some stylistic rules",
            "such as using the present tense",
            "may also have a positive impact on the correctness of the generated code.\n3. Including other information like boundary cases and contextual information in the prompts had no significant effect on the level of correctness of the results",
            "and including excessive information may even have a negative effect."
        ],
        "Future Work":[
            "1. Widening the study to consider conversations between developers and generative AI tools",
            "not just individual interactions.\n2. Extending the analysis to other generative AI tools beyond Copilot",
            "to investigate how generalizable the results about prompt engineering are across different solutions."
        ],
        "Scope from our taxonomy":[
            "Impact"
        ],
        "SDLC stage from waterfall":[
            "Coding: In this phase the whole requirements will be converted to the production environment."
        ],
        "Context":[
            "Professional"
        ],
        "Eligibility score":[

        ],
        "ID":"16"
    },
    {
        "Paper":"Ansible Lightspeed: A Code Generation Service for IT Automation",
        "Authors":[
            "Priyam Sahoo",
            "Saurabh Pujar",
            "Ganesh Nalawade",
            "Richard Gebhardt",
            "Louis Mandel",
            "Luca Buratti"
        ],
        "Affiliation":[
            "Industry"
        ],
        "DOI":"https:\/\/doi.org\/10.1145\/3691620.3695277",
        "Date":2024,
        "Venue":[
            "ASE'24"
        ],
        "Goal":[
            "The goal of the paper is to analyze the usage and effectiveness of the Ansible Lightspeed code completion service",
            "with the aim of determining if users find the service useful and how it can be improved to better fit user needs."
        ],
        "Research Questions":[
            "1. When do users use Ansible Lightspeed?\n2. Do users continue to use the service after trying it once?\n3. Do users modify accepted suggestions?\n4. At what rate do users accept suggestions?\n5. Why do users edit module names in accepted suggestions?"
        ],
        "Key Findings":[
            "1. Users primarily make requests on weekdays",
            "with reduced usage on weekends and holidays",
            "suggesting work-related use.\n2. Retention drops from 44.79% on Day 1 to 13.66% by Day 30. Approximately 36.5% are returning users.\n3. 18.16% of accepted suggestions are later deleted",
            "and 6.62% are heavily modified.\n4. The initial acceptance rate is 65.9%",
            "but this decreases to a Strong Acceptance rate of 49.1% after considering modifications.\n5. Users often edit suggestions to remove Fully Qualified Class Names (FQCN) based on preference",
            "reorganize YAML",
            "or modify implementations to fit their preferred coding style."
        ],
        "Future Work":[
            "1. Generating multiple suggestions for each prompt to better account for user preferences and the fact that there may be multiple valid ways to implement a task.\n2. Expanding the capabilities of the model to generate entire Ansible playbooks or sequences",
            "rather than just individual tasks",
            "to reduce the need for users to manually reorganize and add missing details.\n3. Developing capabilities for Ansible code explanation",
            "debugging",
            "and customization for specific users."
        ],
        "Scope from our taxonomy":[
            "Impact"
        ],
        "SDLC stage from waterfall":[
            "Coding: In this phase the whole requirements will be converted to the production environment."
        ],
        "Context":[
            "Professional"
        ],
        "Eligibility score":[

        ],
        "ID":"17"
    },
    {
        "Paper":"Anticipating User Needs: Insights from Design Fiction on Conversational Agents for Computational Thinking",
        "Authors":[
            "Jacob Penney",
            "Jo\u00e3o Felipe Pimentel",
            "0000",
            "Igor Steinmacher",
            "Marco A Gerosa"
        ],
        "Affiliation":[
            "Academia"
        ],
        "DOI":"https:\/\/doi.org\/10.1007\/978-3-031-54975-5_12",
        "Date":2024,
        "Venue":[
            "CONVERSATIONS 2023"
        ],
        "Goal":[
            "The goal of this paper is to understand instructors' expectations of a conversational agent's capabilities in effectively facilitating the acquisition of computational thinking skills."
        ],
        "Research Questions":[
            "What are instructors' expectations of a programming conversational agent intended to scaffold computational thinking?"
        ],
        "Key Findings":[
            "1. Instructors expect the conversational agent to guide students stepwise through the development of algorithms in natural language",
            "rather than just providing solutions",
            "and to tailor its responses based on the student's educational background and knowledge.\n2. Instructors expect the agent to be able to explain things in the environment that the student might be confused about",
            "such as problem statements",
            "code snippets",
            "diagnostics",
            "and underlying algorithms and concepts",
            "using diverse means like visualization.\n3. Instructors want the agent to collect information about the student's educational history",
            "field of study",
            "doubts\/struggles",
            "and abilities",
            "in order to tailor its responses and provide personalized guidance."
        ],
        "Future Work":[
            "1. Conducting a design fiction study with student populations to understand their perspectives and expectations of a conversational agent for computational thinking.\n2. Performing Wizard of Oz experiments to classify student intentions and how they are expressed in dialogue",
            "to inform the design of the conversational agent.\n3. Designing and prototyping the implementation of the conversational agent",
            "building on the insights gained from the instructor interviews in this paper."
        ],
        "Scope from our taxonomy":[
            "Impact"
        ],
        "SDLC stage from waterfall":[
            "Unspecified"
        ],
        "Context":[
            "Educational"
        ],
        "Eligibility score":[

        ],
        "ID":"18"
    },
    {
        "Paper":"Are Prompt Engineering and TODO Comments Friends or Foes? An Evaluation on GitHub Copilot",
        "Authors":[
            "David Obrien",
            "Sumon Biswas",
            "Sayem Mohammad Imtiaz",
            "Rabe Abdalkareem",
            "Emad Shihab",
            "Hridesh Rajan"
        ],
        "Affiliation":[
            "Academia"
        ],
        "DOI":"https:\/\/doi.org\/10.1145\/3597503.3639176",
        "Date":2024,
        "Venue":[
            "ICSE'24"
        ],
        "Goal":[
            "The goal of this paper is to evaluate the impact of including TODO comments in prompts for the code generation tool GitHub Copilot",
            "and to investigate whether Copilot can be used to automatically repay the technical debts described in these TODO comments."
        ],
        "Research Questions":[
            "RQ1: Does the presence of TODO comments impact the quality of GitHub Copilot\u2019s generated code? \nRQ2: Can GitHub Copilot generations address developer-written TODO comments? \nRQ3: Can TODO comments be modified to enhance prompts that lead to generated code addressing the specified issues?"
        ],
        "Key Findings":[
            "1. Code generative tools can replicate their training data",
            "with 35.36% of the 380 TODO comments in our study having their issues reproduced when directly included in prompts.\n2. Code generative tools can reproduce TODO comments and their issues without explicit instructions.\n3. 21.57% of TODO comment issues were resolved in the generated code",
            "based solely on information from the developer-written docstring.\n4. Themes such as defect-handling",
            "additional support",
            "and improved assumptions indicate areas where code generative tools can help mitigate TODO comment issues.\n5. Omitting \u201cTODO\u201d from comments allows code generative tools to address an additional 10.53% of comments",
            "suggesting the potential of preprocessing to assist with TODO technical debt repayment."
        ],
        "Future Work":[
            "1) Investigating how code generation tools like Copilot can assist with software maintenance and evolution",
            "and how developers interact with and modify the generated code.\n2) Evaluating the quality of the code generated by Copilot",
            "beyond just whether it addresses the TODO comments.\n3) Exploring how TODO comments and the corresponding code change over time",
            "and how code generation tools can assist with software evolution."
        ],
        "Scope from our taxonomy":[
            "Quality"
        ],
        "SDLC stage from waterfall":[
            "Requirement: Is a description of a system behavior to be developed."
        ],
        "Context":[
            "Professional"
        ],
        "Eligibility score":[

        ],
        "ID":"19"
    },
    {
        "Paper":"Assessing the quality of GitHub copilot\u2019s code generation",
        "Authors":[
            "Burak Yetistiren",
            "Isik Ozsoy",
            "Eray Tuzun"
        ],
        "Affiliation":[
            "Academia"
        ],
        "DOI":"https:\/\/doi.org\/10.1145\/3558489.3559072",
        "Date":2022,
        "Venue":[
            "PROMISE'22"
        ],
        "Goal":[
            "The goal of this paper is to systematically evaluate the quality of code generated by GitHub Copilot",
            "a new AI-powered code generation tool",
            "in terms of validity",
            "correctness",
            "and efficiency. The authors also aimed to evaluate the impact of the quality and variety of input parameters provided to GitHub Copilot on the generated code quality."
        ],
        "Research Questions":[
            "RQ1 What is the quality of the code generated by GitHub Copilot? \nRQ1.1 How valid are GitHub Copilot\u2019s code suggestions? \nRQ1.2 How correct are GitHub Copilot\u2019s code suggestions? \nRQ1.3 How efficient are GitHub Copilot\u2019s code suggestions? \nRQ2 What is the effect of using docstrings on the generated code quality? \nRQ3 What is the effect of using appropriate function names on the generated code quality?"
        ],
        "Key Findings":[
            "GitHub Copilot is capable of generating valid code 9 out of 10 times.\nWithout the supervision of a programmer",
            "GitHub Copilot has a lower chance of generating correct code; however",
            "with additional input from the programmer",
            "the correctness of the generated code can be improved.\nThere is no significant difference in terms of code efficiency between the code generated by GitHub Copilot and a human programmer.\nWhile employing GitHub Copilot",
            "the utilization of proper explanations of the given problem is important in terms of acquiring correct and valid code.\nChoosing a meaningful name for a given function positively affects GitHub Copilot\u2019s performance in terms of generating a correct and valid code."
        ],
        "Future Work":[
            "Increase the number of problems and diversify the coverage and the difficulty level of problem scenarios.\nIncrease the number of unit tests for each problem.\nAssess the code quality of GitHub Copilot using maintainability and reliability metrics."
        ],
        "Scope from our taxonomy":[
            "Quality"
        ],
        "SDLC stage from waterfall":[
            "Coding: In this phase the whole requirements will be converted to the production environment.",
            "Maintenance: After the software is already released",
            "it may need some modifications",
            "improvements",
            "errors correction",
            "and refinement accordingly."
        ],
        "Context":[
            "Professional"
        ],
        "Eligibility score":[

        ],
        "ID":"20"
    },
    {
        "Paper":"Better Together? An Evaluation of AI-Supported Code Translation",
        "Authors":[
            "Justin D Weisz",
            "Michael Muller",
            "Steven I Ross",
            "Fernando Martinez",
            "Ibm Argentina",
            "Argentina Stephanie Houde",
            "Mayank Agarwal",
            "John T Richards",
            "Stephanie Houde",
            "Kartik Talamadupula"
        ],
        "Affiliation":[
            "Industry"
        ],
        "DOI":"https:\/\/doi.org\/10.1145\/3490099.3511157",
        "Date":2022,
        "Venue":[
            "IUI'22"
        ],
        "Goal":[
            "The goal of this paper is to empirically examine whether imperfect AI-generated code translations can provide benefits to software engineers conducting code translation tasks",
            "and to understand how the quality and quantity of AI translations impact the work process and outcomes."
        ],
        "Research Questions":[
            "RQ1: Efficacy of AI support. Are software engineers more effective in translating code from one programming language to another when given access to AI-produced translations? How do the quantity and quality of AI translations affect their work? Are they able to improve upon the quality of imperfect translations? \nRQ2: Impact on work process. How does the presence of AI translations affect the code translation work process? Does working with AI-produced translations make the translation task easier? \nRQ3: Benefits & drawbacks. How do software engineers perceive working with AI-produced code?"
        ],
        "Key Findings":[
            "1. Software engineers were more effective in translating code from Java to Python when given access to AI-produced translations",
            "as evidenced by lower error rates and a higher proportion of correctly implemented methods in their translations.\n2. The quality and quantity of AI translations affected the work process",
            "with multiple translations leading to higher frustration and mental demand for participants.\n3. Participants found the AI translations useful and helpful in completing the translation task",
            "even though they recognized the errors present within them."
        ],
        "Future Work":[
            "1. Exploring the tradeoffs between the quality of the generative model's output and the quality of the human-AI collaboration.\n2. Evaluating user interface designs that help people effectively browse",
            "compare",
            "and manage multiple translation alternatives from a generative code model\n3. Investigating how to build appropriate trust between people and generative code models\n4. Exploring ways to make generative code models more explainable and interpretable to users."
        ],
        "Scope from our taxonomy":[
            "Impact"
        ],
        "SDLC stage from waterfall":[
            "Maintenance: After the software is already released",
            "it may need some modifications",
            "improvements",
            "errors correction",
            "and refinement accordingly."
        ],
        "Context":[
            "Professional"
        ],
        "Eligibility score":[

        ],
        "ID":"21"
    },
    {
        "Paper":"Can Developers Prompt? A Controlled Experiment for Code Documentation Generation",
        "Authors":[
            "Hans-Alexander Kruse",
            "Tim Puhlf\u00fcr\u00df",
            "Walid Maalej"
        ],
        "Affiliation":[
            "Academia"
        ],
        "DOI":"https:\/\/doi.org\/10.48550\/arXiv.2408.00686",
        "Date":2024,
        "Venue":[
            "ICSME'24"
        ],
        "Goal":[
            "The goal of this paper is to investigate how well developers can prompt large language models (LLMs) to generate code documentation",
            "and to compare the developer experience between using ad-hoc prompts versus a predefined few-shot prompt."
        ],
        "Research Questions":[
            "RQ1: How well can developers prompt LLMs to generate code documentation compared to a predefined prompt? \nRQ2: How is the developer experience for ad-hoc prompting compared to executing predefined prompts?"
        ],
        "Key Findings":[
            "1. Developers",
            "especially less experienced students",
            "struggled to effectively prompt LLMs to generate high-quality code documentation using ad-hoc prompts.\n2. Predefined few-shot prompts generated higher quality code documentation compared to ad-hoc prompts",
            "especially for less experienced developers.\n3. Both ad-hoc and predefined prompt interactions improved code comprehension",
            "but the generated documentation was often viewed as an intermediate result requiring further iteration and refinement."
        ],
        "Future Work":[
            "1. Understanding developers' prompting skills and preferences",
            "and how to best support them\n2. Identifying the preferences of both code documentation providers and consumers\n3. Creating and testing new quality metrics for evaluating AI-generated code documentation"
        ],
        "Scope from our taxonomy":[
            "Impact"
        ],
        "SDLC stage from waterfall":[
            "Maintenance: After the software is already released",
            "it may need some modifications",
            "improvements",
            "errors correction",
            "and refinement accordingly."
        ],
        "Context":[
            "Professional"
        ],
        "Eligibility score":[

        ],
        "ID":"22"
    },
    {
        "Paper":"Case Study: Using AI-Assisted Code Generation In Mobile Teams",
        "Authors":[
            "Mircea-Serban Vasiliniuc",
            "Adrian Groza"
        ],
        "Affiliation":[
            "Academia"
        ],
        "DOI":"DOI: 10.1109\/ICCP60212.2023.10398656",
        "Date":2023,
        "Venue":[
            "ICCP'23"
        ],
        "Goal":[
            "The goal of this study is to evaluate the performance of AI-assisted programming in actual mobile development teams that are focused on native mobile languages like Kotlin and Swift",
            "with the aim of understanding the impact of using AI-based code generators on technical onboarding",
            "technical stack switch",
            "and technical integration efforts."
        ],
        "Research Questions":[
            "1. How can an AI-based code generator affect the experience when onboarding a new team member or switching technical stacks of an existing colleague?\n2. Can AI-based code generators affect the performance (completion time",
            "correctness) of technical onboarding or technical stack switch tasks?\n3. Can AI-based code generators affect the technical integration efforts of a mobile development team?"
        ],
        "Key Findings":[
            "1. Using AI-assisted code generation tools can significantly reduce the time taken to complete technical onboarding and technical stack switch tasks",
            "with a 35% decrease in task duration for technical onboarding and a 45% decrease for technical stack switch tasks.\n2. While AI-assisted tools can improve task completion time",
            "they may have a slightly negative impact on the correctness of the solutions",
            "as measured by unit test scores.\n3. The impact of AI-assisted tools on technical integration",
            "as measured by the ReviewerScore",
            "was mixed",
            "with better scores for solutions without AI assistance during the technical onboarding phase",
            "but better scores for AI-assisted solutions during the technical stack switch phase.\n4. Effective prompt engineering was found to be crucial for getting the best results from the AI-assisted tools",
            "with participants who were more adept at modifying prompts achieving higher correctness and ReviewerScores."
        ],
        "Future Work":[
            "Not mentioned"
        ],
        "Scope from our taxonomy":[
            "Impact"
        ],
        "SDLC stage from waterfall":[
            "Unspecified"
        ],
        "Context":[
            "Professional"
        ],
        "Eligibility score":[

        ],
        "ID":"23"
    },
    {
        "Paper":"CoLadder: Supporting Programmers with Hierarchical Code Generation in Multi-Level Abstraction",
        "Authors":[
            "Ryan Yen",
            "Jiawen Zhu",
            "Sangho Suh",
            "Haijun Xia",
            "Jian Zhao"
        ],
        "Affiliation":[
            "Academia"
        ],
        "DOI":"https:\/\/doi.org\/10.48550\/arXiv.2310.08699",
        "Date":2023,
        "Venue":[
            "arXiv"
        ],
        "Goal":[
            "The goal of the study is to explore design opportunities for supporting programmers in the intention formation process and the subsequent externalization process when using LLM-driven code assistants."
        ],
        "Research Questions":[
            "1. Whether and how CoLadder supports programmers in their intention formation and externalization process?\n2. Whether CoLadder provides programmers with control when translating intentions into generated code?\n3. Whether and how CoLadder reduces context switching between prompt authoring and results evaluation?\n4. Whether CoLadder enhances prompt-code correspondence for programmers to evaluate generated code?"
        ],
        "Key Findings":[
            "1. CoLadder helps programmers form and externalize their intentions for solving programming tasks through a hierarchical prompt structure and block-based design.\n2. CoLadder provides programmers with control over the translation of their intentions into code by allowing direct manipulation of prompt blocks at multiple levels of abstraction.\n3. CoLadder reduces the cognitive burden of switching between prompt authoring and code evaluation",
            "enhancing programmers' ability to perceive",
            "interpret",
            "and evaluate the LLM's output."
        ],
        "Future Work":[
            "1. Exploring the applicability of CoLadder's design principles to other programming languages beyond scripting languages",
            "such as compiled languages.\n2. Investigating ways to better align the prompt structure with the generated code structure",
            "especially for programming languages or scenarios where the task and code structures can deviate significantly.\n3. Focusing on enhancing programmers' control over the AI-generated code",
            "as well as their control over the program itself",
            "to address the limitations observed in the current study."
        ],
        "Scope from our taxonomy":[
            "Form",
            "Impact"
        ],
        "SDLC stage from waterfall":[
            "Unspecified"
        ],
        "Context":[
            "Professional"
        ],
        "Eligibility score":[

        ],
        "ID":"24"
    },
    {
        "Paper":"Conversing with Copilot: Exploring Prompt Engineering for Solving CS1 Problems Using Natural Language",
        "Authors":[
            "Paul Denny",
            "Nasser Giacaman",
            "Viraj Kumar"
        ],
        "Affiliation":[
            "Academia"
        ],
        "DOI":"https:\/\/doi.org\/10.1145\/3545945.3569823",
        "Date":2023,
        "Venue":[
            "SIGCSE'23"
        ],
        "Goal":[
            "The goal of this paper is to explore the performance of the Copilot AI code generation model on a set of introductory programming problems",
            "as well as the effectiveness of \"prompt engineering\" - modifying the natural language problem descriptions to guide Copilot towards generating correct solutions."
        ],
        "Research Questions":[
            "RQ1: How well does Copilot perform",
            "at the current time",
            "on a public repository of CS1 programming problems? \nRQ2: To what extent do natural language modifications to the problem description lead to the generation of successful solutions after Copilot initially fails? \nRQ3: What commonalities exist amongst problems that Copilot cannot solve",
            "despite reasonable prompt engineering effort?"
        ],
        "Key Findings":[
            "1. Copilot was able to solve around 50% of the 166 CS1 programming problems from the CodeCheck dataset on the first attempt without any prompt engineering.\n2. Prompt engineering",
            "where the natural language problem description is modified to provide more guidance",
            "allowed Copilot to solve an additional 60% of the remaining problems that it initially failed.\n3. The types of problems that Copilot struggled with the most were those involving conceptual understanding (e.g.",
            "swapping neighboring elements) or those with verbose prompts that were difficult for the model to parse."
        ],
        "Future Work":[
            "1. Investigating security and privacy mechanisms to protect confidential data when using AI tools",
            "including prompt sanitization.\n2. Conducting more empirical research on the organizational factors",
            "such as culture and size",
            "that impact the use and adoption of AI tools by practitioners.\n3. Studying the content and structure of training materials and manuals to help practitioners become more effective users of AI tools",
            "particularly around prompt engineering."
        ],
        "Scope from our taxonomy":[
            "Impact"
        ],
        "SDLC stage from waterfall":[
            "Coding: In this phase the whole requirements will be converted to the production environment.",
            "Requirement: Is a description of a system behavior to be developed."
        ],
        "Context":[
            "Educational"
        ],
        "Eligibility score":[

        ],
        "ID":"25"
    },
    {
        "Paper":"Copilot Evaluation Harness: Evaluating LLM-Guided Software Programming",
        "Authors":[
            "Anisha Agarwal",
            "Aaron Chan",
            "Shubham Chandel",
            "Jinu Jang",
            "Shaun Miller",
            "Roshanak Zilouchian Moghaddam",
            "Yevhen Mohylevskyy",
            "Neel Sundaresan",
            "Michele Tufano"
        ],
        "Affiliation":[
            "Industry"
        ],
        "DOI":"https:\/\/doi.org\/10.48550\/arXiv.2402.14261",
        "Date":2024,
        "Venue":[
            "arXiv"
        ],
        "Goal":[
            "The goal of this paper is to introduce the Copilot Evaluation Harness",
            "a comprehensive framework for evaluating the performance of Large Language Models (LLMs) in various software engineering tasks when integrated into Integrated Development Environments (IDEs)."
        ],
        "Research Questions":[
            "RQ1. Model Comparison: How do different LLMs compare to one another when integrated with a coding assistant? \nRQ2. Integration Improvements: What insights can the Copilot Evaluation harness provide engineers to improve the integration of LLM in a coding assistant? \nRQ3. Data Validity: How do our evaluation test cases compare with actual usage of an LLM-powered coding assistant? Do the test cases in our harness reflect how real-world users interact with an LLM-powered coding assistant?"
        ],
        "Key Findings":[
            "1. Comparison of LLM performance: GPT-4 generally outperforms GPT-3.5 and CodeLlama",
            "with some exceptions like C# where GPT-3.5 outperforms the other two.\n2. Insights for improving LLM integration: The evaluation harness can provide insights to improve the integration of LLMs in coding assistants",
            "such as adding instructions to the prompt to not change the focal code.\n3. Validity of the evaluation dataset: The test cases in the dataset are representative of how developers actually use the LLM-powered coding assistant."
        ],
        "Future Work":[
            "Future work on this project involves reporting results for the remaining three evaluation metrics and open-sourcing our data and evaluation code."
        ],
        "Scope from our taxonomy":[
            "Impact",
            "Quality"
        ],
        "SDLC stage from waterfall":[
            "Unspecified"
        ],
        "Context":[
            "Professional"
        ],
        "Eligibility score":[

        ],
        "ID":"26"
    },
    {
        "Paper":"Copilot for Xcode: Exploring AI-Assisted Programming by Prompting Cloud-based Large Language Models",
        "Authors":[
            "Wei Chee",
            "Tan",
            "Shangxin Guo",
            "Man Fai Wong",
            "Ching Nam Hang"
        ],
        "Affiliation":[
            "Academia"
        ],
        "DOI":"https:\/\/doi.org\/10.48550\/arXiv.2307.14349",
        "Date":2023,
        "Venue":[
            "arXiv"
        ],
        "Goal":[
            "The goal of this paper is to introduce and describe an AI-assisted programming tool called Copilot for Xcode that integrates cloud-based large language models with Apple's Xcode IDE to enhance productivity and creativity for software development in the Apple ecosystem."
        ],
        "Research Questions":[
            "How to enable the functionality of GitHub Copilot in Xcode?\nHow good is Copilot for Xcode in tackling real-world programming challenges through AI-assisted programming?"
        ],
        "Key Findings":[
            "1. Copilot for Xcode is an AI-assisted programming tool that integrates cloud-based large language model services with Apple's Xcode IDE to enhance software development productivity and creativity.\n2. Copilot for Xcode enables real-time code suggestions and prompt engineering to facilitate interaction between human programmers and large language models."
        ],
        "Future Work":[
            "Not mentioned"
        ],
        "Scope from our taxonomy":[
            "Form",
            "Impact"
        ],
        "SDLC stage from waterfall":[
            "Unspecified"
        ],
        "Context":[
            "Professional"
        ],
        "Eligibility score":[

        ],
        "ID":"27"
    },
    {
        "Paper":"CoPrompt: Supporting Prompt Sharing and Referring in Collaborative Natural Language Programming",
        "Authors":[
            "Li Feng",
            "Ryan Yen",
            "Mingming Fan",
            "Jian Zhao",
            "Zhicong Lu"
        ],
        "Affiliation":[
            "Academia"
        ],
        "DOI":"https:\/\/doi.org\/10.1145\/3613904.3642212",
        "Date":2024,
        "Venue":[
            "CHI'24"
        ],
        "Goal":[
            "The goal of the research paper is to explore the design of workflows that support programmers in Prompt Co-Engineering",
            "which involves collaboratively refining and sensemaking prompts during natural language (NL) programming."
        ],
        "Research Questions":[
            "1. What are the challenges faced by programmers and their needs in the workfow of prompt coengineering?\n2. How to build a system that would support programmers in their prompt engineering workfow during collaborative NL programming?\n3. How does the system support programmers to understand collaborators\u2019 progress and prompts?\n4. How does the system support programmers to leverage collaborators\u2019 work?\n5. How does the system reduce the repetitive updates of prompts or code?"
        ],
        "Key Findings":[
            "1. The formative study identified user challenges including maintaining group awareness and shared understanding",
            "the repetitive effort of syncing with collaborators' work",
            "inconvenient and disruptive feedback requests",
            "and repetitive copy-pasting for leveraging others' work and sharing information.\n2. Based on our design considerations",
            "authors implemented a prototype",
            "CoPrompt to supports sense-making of the collaboration process and prompt co-engineering.\n3. CoPrompt supported programmers\u2019 sense-making of collaborators\u2019 work by providing the hierarchical overview",
            "generating explanations associated with the prompts",
            "and displaying historical views\n4. All participants found the four mechanisms (refer",
            "request",
            "share",
            "and link) easy to use",
            "intuitive to learn",
            "able to fulfill their requirements",
            "and easy to control",
            "with CoPrompt reducing the need for communication with collaborators and facilitating prompt modifications."
        ],
        "Future Work":[
            "1. Extend the design by considering more complex prompting strategies",
            "such as few-shot prompts.\n2. Investigate the level of generation automation and controllability provided to the programmers"
        ],
        "Scope from our taxonomy":[
            "Form",
            "Impact"
        ],
        "SDLC stage from waterfall":[
            "Unspecified"
        ],
        "Context":[
            "Professional"
        ],
        "Eligibility score":[

        ],
        "ID":"28"
    },
    {
        "Paper":"Defendroid: Real-time Android code vulnerability detection via blockchain federated neural network with XAI",
        "Authors":[
            "Janaka Senanayake",
            "Harsha Kalutarage",
            "Andrei Petrovski",
            "Luca Piras",
            "Omar Al-Kadri"
        ],
        "Affiliation":[
            "Academia"
        ],
        "DOI":"https:\/\/doi.org\/10.1016\/j.jisa.2024.103741",
        "Date":2024,
        "Venue":[
            "Journal of Information Security and Applications"
        ],
        "Goal":[
            "The goal of this paper is to introduce Defendroid",
            "a novel blockchain-based federated neural network model that can detect vulnerabilities in Android source code with high accuracy and efficiency",
            "while also providing explanations for the predictions using Explainable AI (XAI) techniques."
        ],
        "Research Questions":[
            "1. Whether security considerations played a role in Android app developers' development practices.\n2. What are the factors influencing Android app developers' limited attention to secure coding.\n3. How to build a blockchain-based federated neural network enhanced with Explainable Artificial Intelligence  to reduce potential vulnerabilities during development."
        ],
        "Key Findings":[
            "1. A majority of Android app developers (55.9%) do not integrate secure coding practices into their development process.\n2. The main reasons for the lack of attention to secure coding are the prioritization of functionality over security",
            "the need for additional time and resources for manual verification",
            "and the lack of supportive tools for automated security checking.\n3. The study introduced Defendroid",
            "a blockchain-based federated neural network model with XAI",
            "which achieved high accuracy (96%) and F1-score (0.96) in binary and multi-class classification for Android code vulnerability detection.\n4. The Defendroid model was integrated into an Android Studio plugin",
            "enabling real-time vulnerability detection and explanation of the predictions using XAI."
        ],
        "Future Work":[
            "1. Securing the transmission and sharing of model weights within the blockchain",
            "and introducing an incentive mechanism to attract more clients to the federated learning environment.\n2. Incorporating the Defendroid model and its API as a full product",
            "as a user-friendly Android Studio plugin",
            "to further improve its usability and adoption.\n3. Incorporating more principles of XAI and expanding the federated learning environment to a broader audience",
            "which is expected to lead to further improvements in the model's accuracy."
        ],
        "Scope from our taxonomy":[
            "Quality"
        ],
        "SDLC stage from waterfall":[
            "Unspecified"
        ],
        "Context":[
            "Professional"
        ],
        "Eligibility score":[

        ],
        "ID":"29"
    },
    {
        "Paper":"Demystifying Practices, Challenges and Expected Features of Using GitHub Copilot",
        "Authors":[
            "Beiqi Zhang",
            "Peng Liang",
            "Xiyu Zhou",
            "Aakash Ahmad",
            "Muhammad Waseem"
        ],
        "Affiliation":[
            "Academia"
        ],
        "DOI":"https:\/\/doi.org\/10.1142\/S0218194023410048",
        "Date":2023,
        "Venue":[
            "International Journal of Software Engineering and Knowledge Engineering"
        ],
        "Goal":[
            "The goal of this study is to understand the practices",
            "benefits and challenges",
            "and expected features of using GitHub Copilot from the point of view of practitioners in the context of Copilot related SO posts and GitHub discussions."
        ],
        "Research Questions":[
            "RQ1: Practices of Using GitHub Copilot\nRQ1.1: What programming languages are used with GitHub Copilot?\nRQ1.2: What IDEs are used with GitHub Copilot? \nRQ1.3: What technologies are used with GitHub Copilot?\nRQ1.4: What functions are implemented by using GitHub Copilot?\nRQ1.5: What are the purposes of using GitHub Copilot?\n\nRQ2: Benefits",
            "Limitations & Challenges",
            "and Expected Features of Using GitHub Copilot\nRQ2.1: What are the benefits of using GitHub Copilot?\nRQ2.2: What are the limitations & challenges of using GitHub Copilot?\nRQ2.3: What are the expected features of users about GitHub Copilot?"
        ],
        "Key Findings":[
            "\"1. The most common programming languages",
            "IDEs",
            "and technologies used with Copilot are JavaScript",
            "Python",
            "Visual Studio Code",
            "and Node.js.\n2. The primary function implemented by Copilot is data processing",
            "and the main purpose for using it is to generate code.\n3. The biggest benefit of using Copilot is its ability to generate useful code",
            "but the main limitation is the difficulty of integrating it with different development environments. \n4. The most common feature users expect is for Copilot to be integrated with more IDEs beyond just Visual Studio Code.\""
        ],
        "Future Work":[
            "1. Conduct interviews or an online survey to get practitioners' perspectives on using Copilot",
            "to supplement the data collected from repository mining.\n2. Further explore various aspects of Copilot",
            "especially how to improve developers' understanding of the code generated by Copilot.\""
        ],
        "Scope from our taxonomy":[
            "Impact"
        ],
        "SDLC stage from waterfall":[
            "Unspecified"
        ],
        "Context":[
            "Professional"
        ],
        "Eligibility score":[

        ],
        "ID":"30"
    },
    {
        "Paper":"Design Principles for\u00a0Collaborative Generative AI Systems in\u00a0Software Development",
        "Authors":[
            "Johannes Chen",
            "Jan Zacharias"
        ],
        "Affiliation":[
            "Academia"
        ],
        "DOI":"https:\/\/doi.org\/10.1007\/978-3-031-61175-9_23",
        "Date":2024,
        "Venue":[
            "DESRIST 2024"
        ],
        "Goal":[
            "The goal of this research is to equip organizations with comprehensive insights and a robust design framework for effectively leveraging Generative Artificial Intelligence (GAI) systems to enhance their software development processes. The framework is designed to guide the development and implementation of GAI systems across various organizational settings",
            "addressing specific challenges and opportunities unique to each context."
        ],
        "Research Questions":[
            "1. What are specific challenges and opportunities for effectively leveraging Generative Artificial Intelligence (GAI) systems?\n2. What are Design Principles for Collaborative Generative AI Systems in Software Development that would address found challenges and opportunities?"
        ],
        "Key Findings":[
            "1. There are four major problem areas",
            "including usability issues",
            "data privacy issues",
            "hallucination",
            "and insufficient transparency\n2. Four design requirements: user-centricity",
            "data protection",
            "quality control and communication"
        ],
        "Future Work":[
            "Future research could develop and evaluate a prototype for an organization-specific GAI-based programming assistant",
            "guided by proposed design principles",
            "to provide practical validation of theoretical findings."
        ],
        "Scope from our taxonomy":[
            "Form",
            "Impact"
        ],
        "SDLC stage from waterfall":[
            "Unspecified"
        ],
        "Context":[
            "Professional"
        ],
        "Eligibility score":[

        ],
        "ID":"31"
    },
    {
        "Paper":"Designing PairBuddy\u2014A Conversational Agent for Pair Programming",
        "Authors":[
            "Peter Robe",
            "Sandeep Kaur Kuttal"
        ],
        "Affiliation":[
            "Academia"
        ],
        "DOI":"https:\/\/doi.org\/10.1145\/3498326",
        "Date":2022,
        "Venue":[
            "TOCHI'22"
        ],
        "Goal":[
            "The goal of this publication is to design and evaluate a conversational agent called \"PairBuddy\" that can serve as an interactive programming partner for pair programming."
        ],
        "Research Questions":[
            "1. How can a conversational agent be designed to effectively simulate a human partner in pair programming?\n2. What are the trade-offs of replacing a human partner with a conversational agent in pair programming?"
        ],
        "Key Findings":[
            "1.  Anthropomorphic design space of PairBuddy arose from integrating diverse interface and\ninteraction mechanisms (embodiment",
            "dialogue styles",
            "and agent actions) and programmer\ncharacteristics (technical skills and soft skills).\n2. PairBuddy was an effective pair programming partner and was enjoyed by study participants."
        ],
        "Future Work":[
            "1. Explore additional avenues to provide a personalized PairBuddy experience.\n2. In future studies",
            "we will investigate whether anthropomorphic features are appropriate\nfor PairBuddy.\n3. Future work must investigate the range of programming experience within both educational and professional settings."
        ],
        "Scope from our taxonomy":[
            "Form",
            "Impact"
        ],
        "SDLC stage from waterfall":[
            "Unspecified"
        ],
        "Context":[
            "Educational"
        ],
        "Eligibility score":[

        ],
        "ID":"32"
    },
    {
        "Paper":"Developers' Perspective on Today's and Tomorrow's Programming Tool Assistance: A Survey",
        "Authors":[
            "Peng Kuang",
            "Emma S\u00f6derberg",
            "Martin H\u00f6st"
        ],
        "Affiliation":[
            "Academia"
        ],
        "DOI":"https:\/\/doi.org\/10.1145\/3660829.3660848",
        "Date":2024,
        "Venue":[
            "Programming '24"
        ],
        "Goal":[
            "The goal of this paper is to investigate developers' perceptions of current and future programming tool assistance",
            "including their experiences with program analysis",
            "attitudes towards AI\/ML",
            "eye-tracking",
            "and gamification",
            "as well as their main pain points in programming."
        ],
        "Research Questions":[
            "RQ 1: What are developers' perception on tool assistance today?\nRQ 2: What are developers' perception on directions for future tool assistance?"
        ],
        "Key Findings":[
            "1. Program analysis is broadly used in practice",
            "and the majority of users find it useful",
            "though they report frustrations with setup and results. AI-based tools are making a big entry",
            "with the majority of participants asked about this topic reporting use for a range of programming tasks. Our participants report a variety of pain points",
            "with code comprehension being the most prominent. English proficiency is indicated as impacting understanding",
            "but English is at the same time reported as part of understanding in programming.\n2. Participants are generally positive toward AI-based assistance and see opportunities for several improvements of existing tools using this technology. They are wary of eye-tracking",
            "with concerns about privacy and inconvenient equipment",
            "while being primarily negative about gamification. They see AI-based tools as the main direction to focus on for the future."
        ],
        "Future Work":[
            "1. Improved support for code comprehension\n2. Improved reliability and integration of AI-based tools\n3. Continued exploration of eye tracking with a focus on what developers want"
        ],
        "Scope from our taxonomy":[
            "Impact"
        ],
        "SDLC stage from waterfall":[
            "Unspecified"
        ],
        "Context":[
            "Professional"
        ],
        "Eligibility score":[

        ],
        "ID":"33"
    },
    {
        "Paper":"Discovering the Syntax and Strategies of Natural Language Programming with Generative Language Models",
        "Authors":[
            "Ellen Jiang",
            "Edwin Toh",
            "Alejandra Molina",
            "Kristen Olson",
            "Claire Kayacik",
            "Aaron Donsbach",
            "Carrie J Cai",
            "Michael Terry"
        ],
        "Affiliation":[
            "Academia",
            "Industry"
        ],
        "DOI":"https:\/\/doi.org\/10.1145\/3491102.3501870",
        "Date":2022,
        "Venue":[
            "CHI'22"
        ],
        "Goal":[
            "The goal of the study was to understand the user experience of using a large generative language model to produce code from natural language descriptions."
        ],
        "Research Questions":[
            "1) What is the user experience of using a large language model to produce code from natural language descriptions?\n2) How may this new class of large language models affect users' software development practices?"
        ],
        "Key Findings":[
            "1. The natural language code synthesis capabilities can be useful for certain tasks (e.g.",
            "creating boilerplate code",
            "or doing the equivalent\nof an API lookup)",
            "2. Users can encounter challenges in areas like learning the model\u2019s natural language \u201csyntax\u201d",
            "knowing what users can reliably ask of the model",
            "debugging the model when it doesn\u2019t produce the desired results"
        ],
        "Future Work":[
            "1) Providing suggestions and examples to help users learn the \"syntax\" of the model and understand what types of requests it can reliably handle.\n2) Enabling more structured",
            "conversational interactions to help users refine vague or ambiguous requests.\n3) Improving model interpretability and providing AI onboarding to help users build accurate mental models of the model's capabilities."
        ],
        "Scope from our taxonomy":[
            "Form",
            "Impact"
        ],
        "SDLC stage from waterfall":[
            "Unspecified"
        ],
        "Context":[
            "Professional"
        ],
        "Eligibility score":[

        ],
        "ID":"34"
    },
    {
        "Paper":"Documentation Matters: Human-Centered AI System to Assist Data Science Code Documentation in Computational Notebooks",
        "Authors":[
            "April Yi Wang",
            "Dakuo Wang",
            "Jaimie Drozdal",
            "Michael Muller",
            "Soya Park",
            "Justin D. Weisz",
            "Xuye Liu",
            "Lingfei Wu",
            "Casey Dugan"
        ],
        "Affiliation":[
            "Academia",
            "Industry"
        ],
        "DOI":"https:\/\/doi.org\/10.1145\/3489465",
        "Date":2022,
        "Venue":[
            "ACM Transactions on Computer-Human Interaction"
        ],
        "Goal":[
            "The goal of the study is to design and implement an automated documentation generation system called Themisto to support data scientists in creating better-documented computational narratives in Jupyter Notebooks."
        ],
        "Research Questions":[
            "1. What are the characteristics of well-documented computational notebooks?\n2. How can AI be used to assist data scientists in creating documentation for computational notebooks?"
        ],
        "Key Findings":[
            "1. Themisto",
            "the automated documentation generation system",
            "reduced the time for data scientists to create documentation",
            "reminded them to document code they would have ignored",
            "and improved their satisfaction with their computational notebooks.\n2. The quality of the documentation produced with Themisto was about the same as what data scientists produced on their own.\n3. The three different approaches used by Themisto (deep-learning-based",
            "query-based",
            "and prompt-based) were perceived as suitable for different scenarios by the participants.\n4. Most participants indicated that they would like to use Themisto in their future data science projects."
        ],
        "Future Work":[
            "1. Exploring the long-term effects of using Themisto",
            "including whether it helps users learn more about documentation or leads to over-reliance on the AI system.\n2. Investigating hybrid and adapted approaches to code summarization",
            "rather than fully automatic approaches.\n3. Customizing the documentation recommendations based on the specific usage scenarios and needs of data science workers",
            "taking a user-centered approach.\n4. Exploring the possibility of generating code from descriptive text",
            "as some data scientists may write documentation before writing code."
        ],
        "Scope from our taxonomy":[
            "Form",
            "Impact"
        ],
        "SDLC stage from waterfall":[
            "Maintenance: After the software is already released",
            "it may need some modifications",
            "improvements",
            "errors correction",
            "and refinement accordingly."
        ],
        "Context":[
            "Professional"
        ],
        "Eligibility score":[

        ],
        "ID":"35"
    },
    {
        "Paper":"Enhancing Code Completion with Implicit Feedback",
        "Authors":[
            "Haonan Jin",
            "Yu Zhou",
            "Yasir Hussain"
        ],
        "Affiliation":[
            "Academia"
        ],
        "DOI":"DOI: 10.1109\/QRS60937.2023.00030",
        "Date":2023,
        "Venue":[
            "QRS'23"
        ],
        "Goal":[
            "The goal of this paper is to introduce a novel framework called EHOPE (Enhance Code Completion with Implicit Feedback) that aims to enhance the performance of token-level code completion by leveraging users' feedback information."
        ],
        "Research Questions":[
            "RQ1: How effective is the EHOPE method to complete code for given code segments in general?\nRQ2: How does the utilization of feedback information enhance the performance of EHOPE in code completion? Specifically",
            "how does the size of the feedback repository affect the performance of code completion?\nRQ3: How do LSTM and BERT contribute to EHOPE respectively?"
        ],
        "Key Findings":[
            "1. EHOPE",
            "a novel framework",
            "enhances token-level code completion accuracy by integrating programmers' feedback information through the use of LSTM and pre-trained model BERT.\n2. Empirical experiments show that the recommendation performance of EHOPE steadily and substantially improves as the feedback data increases",
            "outperforming the baseline code completion systems N-gram and CodeGRU.\n3. The LSTM model trained with feedback data generates its own list of recommendations",
            "which can be merged with the initial token list to improve the effectiveness of code completion."
        ],
        "Future Work":[
            "1. Developing EHOPE into a full-fledged tool that can be integrated as a plugin for mainstream IDEs to provide better support for programming.\n2. Extending the EHOPE approach to other recommendation scenarios in software engineering beyond just code completion.\n3. Exploring techniques for data filtering and noise reduction to address the reliability issues with user feedback.\n4. Using active learning techniques to overcome the initial lack of data in the feedback repository and provide a more robust approach.\""
        ],
        "Scope from our taxonomy":[
            "Quality"
        ],
        "SDLC stage from waterfall":[
            "Coding: In this phase the whole requirements will be converted to the production environment."
        ],
        "Context":[
            "Professional"
        ],
        "Eligibility score":[

        ],
        "ID":"36"
    },
    {
        "Paper":"Evaluating Human-AI Partnership for LLM-based Code Migration",
        "Authors":[
            "Behrooz Omidvar-Tehrani",
            "Anmol Anubhai"
        ],
        "Affiliation":[
            "Industry"
        ],
        "DOI":"https:\/\/doi.org\/10.1145\/3613905.3650896",
        "Date":2024,
        "Venue":[
            "CHI EA'24"
        ],
        "Goal":[
            "The goal of the paper is to understand how developers interact with and collaborate with an LLM-based code migration tool",
            "specifically focusing on the roles of humans versus AI and how users can build trust in the outcomes generated by the AI."
        ],
        "Research Questions":[
            "RQ1: What role do developers expect LLMs to play in code migration? What is the ideal level of human involvement and oversight at different stages of the LLM action? \nRQ2: How can developers build trust with the outputs of the AI-generated code modifications for code migration?"
        ],
        "Key Findings":[
            "1: Developers desire involvement not only during code review but also in directing the AI to produce the desired output using their own knowledge and the AI\u2019s prior experience. \n2: When performing code reviews",
            "developers hold the AI-generated code to the same standards as code written by human teammates. \n3: Varying outcomes from different LLM models necessitate tailored approaches to help developers establish trust in the AI output."
        ],
        "Future Work":[
            "Further research should be done to explore the best way to present testing suite results information to participants."
        ],
        "Scope from our taxonomy":[
            "Impact"
        ],
        "SDLC stage from waterfall":[
            "Maintenance: After the software is already released",
            "it may need some modifications",
            "improvements",
            "errors correction",
            "and refinement accordingly."
        ],
        "Context":[
            "Professional"
        ],
        "Eligibility score":[

        ],
        "ID":"37"
    },
    {
        "Paper":"Expectation vs. Experience: Evaluating the Usability of Code Generation Tools Powered by Large Language Models",
        "Authors":[
            "Priyan Vaithilingam",
            "Tianyi Zhang",
            "Elena L Glassman"
        ],
        "Affiliation":[
            "Academia",
            "Industry"
        ],
        "DOI":"https:\/\/doi.org\/10.1145\/3491101.3519665",
        "Date":2022,
        "Venue":[
            "CHI EA'22"
        ],
        "Goal":[
            "The goal of this paper is to understand how programmers use and perceive a large language model-based code generation tool",
            "specifically GitHub Copilot",
            "in their programming workflow."
        ],
        "Research Questions":[
            "RQ1: How does using Copilot affect the programming experience? \nRQ2: How do users recognize errors in code generated by Copilot? \nRQ3: What coping mechanisms do users employ when they find errors in code generated by Copilot? \nRQ4: What are the obstacles and limitations that can prevent adoption of Copilot?"
        ],
        "Key Findings":[
            "1. Participants using Copilot failed tasks more often than those using Intellisense. However",
            "tasks completed with Copilot were done more quickly. Despite this",
            "19 out of 24 participants preferred Copilot",
            "and 23 out of 24 found it more helpful than Intellisense.\n2. Participants struggled to understand code generated by Copilot. When errors occurred",
            "frequent context switching to debugging mode added significant mental strain.\n3. Participants coped with incorrect Copilot code by either attempting to repair it or discarding it entirely and searching for online solutions.\n4. Major obstacles included difficulty in understanding and assessing the correctness of generated code",
            "underestimating the effort required to fix bugs",
            "and the brittleness and ambiguity of using comments or prompts as specifications for Copilot."
        ],
        "Future Work":[
            "1. Providing explanations and annotations within the generated code to help users better understand it.\n2. Automatically generating test cases and test data to help users validate the generated code and identify issues.\n3. Exploring interaction mechanisms that better support task decomposition",
            "as this strategy helped some participants be more successful with Copilot.\n4. Integrating online search capabilities with the code generation features of Copilot",
            "to allow users to compare AI-generated code with examples from the internet and identify the best solution."
        ],
        "Scope from our taxonomy":[
            "Impact"
        ],
        "SDLC stage from waterfall":[
            "Unspecified"
        ],
        "Context":[
            "Professional"
        ],
        "Eligibility score":[

        ],
        "ID":"38"
    },
    {
        "Paper":"Exploring Interaction Patterns for Debugging: Enhancing Conversational Capabilities of AI-assistants",
        "Authors":[
            "Bhavya Chopra",
            "Microsoft Bengaluru",
            "Yasharth Bajpai",
            "Param Biyani",
            "Gustavo Soares",
            "Arjun Radhakrishna",
            "Chris Parnin",
            "Sumit Gulwani"
        ],
        "Affiliation":[
            "Industry"
        ],
        "DOI":"https:\/\/doi.org\/10.48550\/arXiv.2402.06229",
        "Date":2024,
        "Venue":[
            "arXiv"
        ],
        "Goal":[
            "The goal of this paper is to enhance the conversational capabilities of AI assistants for debugging tasks by incorporating interaction patterns beyond the standard question-answer format",
            "such as insert expansion and guided turn-taking",
            "in order to improve the effectiveness and fluidity of conversations between developers and AI assistants."
        ],
        "Research Questions":[
            "1. Can leveraging interaction patterns like insert expansion improve the effectiveness of an AI assistant for debugging tasks?\n2. Can facilitating turn-taking and utilizing debugging workflows further enhance the conversational experience and task performance?"
        ],
        "Key Findings":[
            "1. Equipping the AI assistant with knowledge of interaction patterns and debugging workflows leads to a 5-times higher success rate in task completion compared to a baseline AI assistant.\n2. The AI assistant's collaborative behavior",
            "which involves guiding the user through the debugging process and leveraging the user's expertise",
            "leads to deeper investigation of bugs and correct identification of their root cause.\n3. The AI assistant's ability to provide actionable plans and guide the user in using debugger features within the IDE lowers conversational barriers and improves the user experience.\n4. The AI assistant's use of interaction patterns beyond the traditional question-answer pairs",
            "such as insert expansion",
            "and its ability to facilitate turn-taking",
            "contribute to the improved effectiveness and fluidity of the conversations."
        ],
        "Future Work":[
            "1. Exploring the use of additional interaction patterns beyond just question-answer and insert expansion to further enhance the conversational capabilities of the AI assistant for debugging.\n2. Personalizing the AI assistant's responses based on the developer's level of expertise to better cater to the varied needs of beginners versus experts.\n3. Enabling deeper integration of the AI assistant within the IDE",
            "allowing it to perform automated actions like setting breakpoints and stepping through code",
            "and improving its ability to reference and understand the user's code."
        ],
        "Scope from our taxonomy":[
            "Form",
            "Impact"
        ],
        "SDLC stage from waterfall":[
            "Maintenance: After the software is already released",
            "it may need some modifications",
            "improvements",
            "errors correction",
            "and refinement accordingly."
        ],
        "Context":[
            "Professional"
        ],
        "Eligibility score":[

        ],
        "ID":"39"
    },
    {
        "Paper":"Exploring the Learnability of Program Synthesizers by Novice Programmers",
        "Authors":[
            "Dhanya Jayagopal",
            "Justin Lubin",
            "Sarah E. Chasins"
        ],
        "Affiliation":[
            "Academia"
        ],
        "DOI":"https:\/\/doi.org\/10.1145\/3526113.3545659",
        "Date":2022,
        "Venue":[
            "UIST'22"
        ],
        "Goal":[
            "The goal of the paper is to understand how to make program synthesizers more learnable for novice programmers",
            "in order to inform design guidelines that can make synthesizers more approachable and impactful for a broader class of users",
            "including non-CS students."
        ],
        "Research Questions":[
            "1. What aspects of program synthesizers contribute to and detract from their learnability by novice programmers? \n2. What are key design opportunities to make synthesis tools more learnable for novice programmers."
        ],
        "Key Findings":[
            "1. Novice programmers prefer tools that allow task completion using both synthesis and manual coding strategies.  \n2. Incidental specifications ease novice on-ramping to synthesizers.  \n3. Triggerless synthesis initiation is more learnable but requires effective result communication.  \n4. User-triggered synthesis tools should guide users on how many examples are needed for tasks.  \n5. Misconceptions",
            "like larger specifications improving performance",
            "require proactive tool design to address them.  \n6. Reducing specification burdens can include incidental specification collection.  \n7. Novices struggle with distinguishing synthesis success from task progress",
            "needing documentation or scaffolding.  \n8. Awareness of pre-existing strategies benefits synthesis design.  \n9. Feedback on failures and incorrect outputs aids novice understanding.  \n10. Synthesis encourages reading and tracing code",
            "supporting comprehension skills.  \n11. Refining specifications while using synthesis enhances novic"
        ],
        "Future Work":[
            "1. Exploring the learnability of program synthesizers for non-CS students and domain experts who may want to use them for specific tasks without learning programming.\n2. Investigating the impact of program synthesizers on novice programmers' learning outcomes in computer science education",
            "potentially through longitudinal studies.\n3. Conducting additional qualitative studies with different structures",
            "as well as quantitative studies to further explore the hypotheses generated from this qualitative analysis."
        ],
        "Scope from our taxonomy":[
            "Impact"
        ],
        "SDLC stage from waterfall":[
            "Unspecified"
        ],
        "Context":[
            "Educational"
        ],
        "Eligibility score":[

        ],
        "ID":"40"
    },
    {
        "Paper":"From \"Ban It Till We Understand It\" to \"Resistance is Futile\": How University Programming Instructors Plan to Adapt as More Students Use AI Code Generation and Explanation Tools such as ChatGPT and GitHub Copilot",
        "Authors":[
            "Sam Lau",
            "Philip J Guo"
        ],
        "Affiliation":[
            "Academia"
        ],
        "DOI":"https:\/\/doi.org\/10.1145\/3568813.3600138",
        "Date":2023,
        "Venue":[
            "ICER'23"
        ],
        "Goal":[
            "The goal of this paper is to present the first empirical study of computing instructors' perspectives on how they plan to adapt their courses in response to the growing prevalence of AI code generation and explanation tools",
            "and to spur conversations within the computing education community about how to work with these tools in effective",
            "equitable",
            "and ethical ways."
        ],
        "Research Questions":[
            "1. How are computing educators planning to adapt their courses in response to the growing proliferation of AI code generation and explanation tools?\n2. What are the instructors' perspectives on AI code generation and explanation tools?"
        ],
        "Key Findings":[
            "1. In the short term",
            "instructors plan to address cheating concerns by increasing the weight of exam scores",
            "banning AI tools",
            "and educating students on AI capabilities and limitations.\n2. Longer-term strategies to resist AI tools include focusing on fundamental skills",
            "addressing ethical and equity concerns",
            "designing AI-proof assignments",
            "and using alternative exam formats like paper",
            "oral",
            "and video-based assessments.\n3. Plans to embrace AI tools over the long term involve providing personalized support to students",
            "assisting instructors with time-consuming tasks",
            "focusing on code reading and critique",
            "creating open-ended design tasks",
            "and encouraging collaboration between students and AI."
        ],
        "Future Work":[
            "1. Theory-building: Investigate mental models formed by novices and experts regarding AI-generated code and AI tool processes",
            "identifying strategies they use for validating outputs. This will inform methods to help novices develop effective mental models for AI tool usage.\n2. Scaffolding novice understanding: Incorporate pedagogical elements in AI tools to clarify code suggestions and processes",
            "drawing on techniques like grounded abstraction matching from HCI and Explainable AI research.\n3. Tailoring AI coding tools for pedagogy: Adapt AI tools to support learning by encouraging deeper cognitive engagement rather than providing direct answers.\n4. Adapting IDEs for AI-aware pedagogy: Redesign IDEs to promote skills such as code comprehension and critique",
            "and discourage overreliance on AI-generated code.\n5. Equity and access: Develop curricula that integrate AI tools equitably",
            "minimizing the digital divide and ensuring equal access to modern tools for all students.\n6. Efficacy studie"
        ],
        "Scope from our taxonomy":[
            "Impact",
            "Other"
        ],
        "SDLC stage from waterfall":[
            "Unspecified"
        ],
        "Context":[
            "Educational"
        ],
        "Eligibility score":[

        ],
        "ID":"41"
    },
    {
        "Paper":"Generation Probabilities Are Not Enough: Uncertainty Highlighting in AI Code Completions",
        "Authors":[
            "Helena Vasconcelos",
            "Gagan Bansal",
            "Adam Fourney",
            "Q. Vera Liao",
            "Jennifer Wortman Vaughan"
        ],
        "Affiliation":[
            "Academia",
            "Industry"
        ],
        "DOI":"https:\/\/doi.org\/10.1145\/3702320",
        "Date":2024,
        "Venue":[
            "ACM Transactions on Computer-Human Interaction"
        ],
        "Goal":[
            "The goal of the study was to explore whether conveying information about uncertainty via highlights in AI-powered code completion tools can enable programmers to more quickly and accurately produce code when collaborating with such tools."
        ],
        "Research Questions":[
            "1. How much benefit uncertainty-highlighting provides to participants in terms of their task performance and efficiency\n2. Whether highlighting would affect how much participants have to edit or add to the code\n3. What are participants\u2019 subjective preferences for the tools"
        ],
        "Key Findings":[
            "1. Highlighting tokens with the highest predicted likelihood of being edited by a programmer leads to faster task completion and more targeted edits",
            "and is subjectively preferred by study participants.\n2. In contrast",
            "highlighting tokens according to their probability of being generated does not provide any benefit over the baseline with no highlighting.\n3. Participants prefer highlights that are granular",
            "informative",
            "interpretable",
            "and not overwhelming."
        ],
        "Future Work":[
            "1. Developing a more general-purpose edit model that can be applied to open-world scenarios",
            "beyond the closed-world model used in this study.\n2. Exploring ways to better capture the changes people would want to make to code",
            "beyond just the edits they actually make.\n3. Using more representative coding tasks and considering a broader range of code quality metrics beyond just algorithmic correctness.\n4. Examining how the results might differ with a more diverse set of participants",
            "including students and those with less programming experience."
        ],
        "Scope from our taxonomy":[
            "Form",
            "Impact"
        ],
        "SDLC stage from waterfall":[
            "Coding: In this phase the whole requirements will be converted to the production environment."
        ],
        "Context":[
            "Professional"
        ],
        "Eligibility score":[

        ],
        "ID":"42"
    },
    {
        "Paper":"Github copilot in the classroom: learning to code with AI assistance",
        "Authors":[
            "Ben Puryear",
            "Gina Sprint"
        ],
        "Affiliation":[
            "Academia"
        ],
        "DOI":"https:\/\/dl.acm.org\/doi\/abs\/10.5555\/3575618.3575622",
        "Date":2022,
        "Venue":[
            "Journal of Computing Sciences in Colleges"
        ],
        "Goal":[
            "The goal of the study is to describe the experiences of alumni serving as adjunct faculty to teach introductory computer science labs",
            "and to evaluate the effectiveness of this approach."
        ],
        "Research Questions":[
            "1. Could Copilot be a viable tool for introductory programming students?\n2. Could Copilot solutions earn a passing grade score?\n3. Could Copilot solutions be flagged as plagiarism?"
        ],
        "Key Findings":[
            "1. Copilot is an easy-to-use and accurate-enough tool that novice programmers can use to write the majority of the code required to solve fundamental programming tasks. \n2. For an introductory data science course with Python",
            "we found that Copilot-generated solutions earned a human-graded score between 68% and 95%. \n3. When compared to actual student-authored code",
            "Copilot had very low code similarity scores. This suggests AI-generated solutions are unique and difficult to distinguish from human-authored solutions."
        ],
        "Future Work":[
            "1. Observing how novice programmers utilize Copilot and analyzing similarities among code produced by different students using Copilot for the same assignment."
        ],
        "Scope from our taxonomy":[
            "Impact"
        ],
        "SDLC stage from waterfall":[
            "Coding: In this phase the whole requirements will be converted to the production environment."
        ],
        "Context":[
            "Educational"
        ],
        "Eligibility score":[

        ],
        "ID":"43"
    },
    {
        "Paper":"Grounded Copilot: How Programmers Interact with Code-Generating Models",
        "Authors":[
            "Shraddha Barke",
            "San Diego",
            "Michael B James",
            "Nadia Polikarpova"
        ],
        "Affiliation":[
            "Academia"
        ],
        "DOI":"https:\/\/doi.org\/10.1145\/3586030",
        "Date":2023,
        "Venue":[
            "Proceedings of the ACM on Programming Languages"
        ],
        "Goal":[
            "The goal of this paper is to explore how programmers interact with code-generating AI models like Github Copilot",
            "in order to inform the design of future programming assistant tools."
        ],
        "Research Questions":[
            "1. What kinds of tasks do programmers need assistance with? \n2. How do programmers prefer to communicate their intent to the tool? \n3. How do they validate the generated code to determine its correctness and how do they cope with errors?"
        ],
        "Key Findings":[
            "1. Grounded theory of Copilot-assisted programming: User interactions with Copilot can be classified into two modes\u2014acceleration and exploration\u2014akin to the two systems of thought in dual-process theories of cognition.\n2. Based on our theory",
            "we provide design recommendations for future programming assistants",
            "including Control over the context",
            "Awareness of the interaction mode",
            "Exploring multiple suggestions."
        ],
        "Future Work":[
            "Not mentioned"
        ],
        "Scope from our taxonomy":[
            "Impact"
        ],
        "SDLC stage from waterfall":[
            "Unspecified"
        ],
        "Context":[
            "Professional"
        ],
        "Eligibility score":[

        ],
        "ID":"44"
    },
    {
        "Paper":"How Do Data Analysts Respond to AI Assistance? A Wizard-of-Oz Study",
        "Authors":[
            "Madeleine Grunde-Mclaughlin",
            "Jeffrey Heer"
        ],
        "Affiliation":[
            "Academia"
        ],
        "DOI":"https:\/\/doi.org\/10.1145\/3613904.3641891",
        "Date":2024,
        "Venue":[
            "CHI'24"
        ],
        "Goal":[
            "The goal of the paper is to explore the design of AI-based data analysis assistants that can provide both execution and planning assistance to make data analysis more robust and reliable and to provide design guidelines based on the study findings."
        ],
        "Research Questions":[
            "RQ 1 - Scope: What types of specifc planning assistance content could an assistant provide?\nRQ 2 - Helpfulness: What data analysis suggestions do analysts find helpful",
            "and under what circumstances?\nRQ 3 - Impact: How do suggestions impact analysts\u2019 workfows? To what extent do analysts prioritize their own ideas or adopt suggestions and explore alternative approaches?"
        ],
        "Key Findings":[
            "1. Categories of suggestions: Domain Background",
            "Data Wrangling Assistance",
            "Conceptual Model Formulation",
            "Operationalizing Constructs",
            "Choosing the Statistical Model",
            "Model Results Interpretation",
            "High-Level Planning",
            "Execution Assistance.\n2. Analysts preferred suggestions aligned with their analysis plans and backgrounds",
            "valued both execution and planning assistance (with planning requiring more cognitive effort)",
            "and found explanations in code",
            "comments",
            "and natural language helpful for understanding suggestions",
            "particularly when well-timed to their tasks.\n3. Well-timed and contextual planning assistance helped analysts make alternative decisions",
            "though mismatched suggestions were often rejected",
            "and there were potential drawbacks such as distraction or over-reliance",
            "with impacts varying by individual",
            "assistance type",
            "and timing."
        ],
        "Future Work":[
            "1. Future work should explore how this style of assistance can be support for novice data analysts."
        ],
        "Scope from our taxonomy":[
            "Form",
            "Impact"
        ],
        "SDLC stage from waterfall":[
            "Unspecified"
        ],
        "Context":[
            "Professional"
        ],
        "Eligibility score":[

        ],
        "ID":"45"
    },
    {
        "Paper":"How Novices Use LLM-Based Code Generators to Solve CS1 Coding Tasks in a Self-Paced Learning Environment",
        "Authors":[
            "Majeed Kazemitabaar",
            "Xinying Hou",
            "Austin Henley",
            "Barbara J Ericson",
            "David Weintrop",
            "Tovi Grossman",
            "How Novices"
        ],
        "Affiliation":[
            "Academia",
            "Industry"
        ],
        "DOI":"https:\/\/doi.org\/10.1145\/3631802.3631806",
        "Date":2023,
        "Venue":[
            "Koli Calling'23"
        ],
        "Goal":[
            "The goal of this paper is to understand how novice programmers use and interact with LLM-based AI code generators",
            "such as OpenAI Codex",
            "when learning to write code in a self-paced Python programming learning environment."
        ],
        "Research Questions":[
            "1. How do novices use and interact with LLM-based Code Generators when learning to write code by practicing CS1 coding tasks in a self-paced learning environment?\n2. What coding approaches do novice learners employ when they have access to LLM-based code generators to solve programming problems? How do these approaches differently impact learning outcomes measured by retention post-tests?"
        ],
        "Key Findings":[
            "1. Novice learners used the AI code generator (Codex) in various contexts",
            "including at the beginning of a task",
            "after clearing the editor",
            "after manual coding",
            "after a previous Codex usage",
            "and while already having a solution.\n2. Learners used Codex to generate the entire solution",
            "generate new subgoals",
            "and fix existing code. They crafted prompts with varying levels of detail and clarity",
            "which affected the quality of the generated code.\n3. The study identified four distinct coding approaches used by learners when writing code with Codex: AI Single Prompt",
            "AI Step-by-Step",
            "Hybrid",
            "and Manual. The AI Single Prompt approach",
            "where learners prompted Codex once to generate the entire solution",
            "resulted in the highest correctness scores on code-authoring tasks but the lowest correctness scores on subsequent code-modification tasks."
        ],
        "Future Work":[
            "1. Future studies with more rigorous techniques such as thinkaloud or eye-tracking studies are required to properly understand how learners verify and check AI-generated code.\n2.  More in-depth studies are required to explore the effect of coding approaches on individual learning outcomes."
        ],
        "Scope from our taxonomy":[
            "Impact"
        ],
        "SDLC stage from waterfall":[
            "Unspecified"
        ],
        "Context":[
            "Educational"
        ],
        "Eligibility score":[

        ],
        "ID":"46"
    },
    {
        "Paper":"How Readable is Model-generated Code? Examining Readability and Visual Inspection of GitHub Copilot",
        "Authors":[
            "Naser Al Madi"
        ],
        "Affiliation":[
            "Academia"
        ],
        "DOI":"https:\/\/doi.org\/10.1145\/3551349.3560438",
        "Date":2022,
        "Venue":[
            "ASE'22"
        ],
        "Goal":[
            "The goal of this paper is to study the readability and visual inspection of code generated by the GitHub Copilot AI tool",
            "in comparison to code written by human programmers."
        ],
        "Research Questions":[
            "1. RQ1: How readable is Copilot-generated code compared to code written completely by human programmers?\n2. RQ2: How well do programmers inspect Copilot generated code?"
        ],
        "Key Findings":[
            "1. Copilot-generated code is generally comparable in complexity and readability to code written by human pair programmers.\n2. Programmers spend less time visually inspecting Copilot-generated code compared to human-written code",
            "suggesting a risk of automation bias.\n3. While Copilot-generated code is largely comparable to human-written code",
            "it appears to be more difficult to comprehend according to the Halstead difficulty metric."
        ],
        "Future Work":[
            "1. Developing a tool or layer on top of Copilot to filter out buggy and non-optimal suggestions",
            "in order to reduce the liability of using Copilot in software projects.\n2. Conducting further studies using the study design from this paper but exploring more diverse programming tasks and involving more heterogeneous participants in order to more comprehensively compare Copilot with humans as an AI pair programmer."
        ],
        "Scope from our taxonomy":[
            "Impact",
            "Quality"
        ],
        "SDLC stage from waterfall":[
            "Coding: In this phase the whole requirements will be converted to the production environment."
        ],
        "Context":[
            "Professional"
        ],
        "Eligibility score":[

        ],
        "ID":"47"
    },
    {
        "Paper":"IDA: Breaking Barriers in No-code UI Automation Through Large Language Models and Human-Centric Design",
        "Authors":[
            "Segev Shlomov",
            "Avi Yaeli",
            "Sami Marreed",
            "Sivan Schwartz",
            "Netanel Eder",
            "Offer Akrabi",
            "Sergey Zeltyn"
        ],
        "Affiliation":[
            "Academia",
            "Industry"
        ],
        "DOI":"https:\/\/doi.org\/10.48550\/arXiv.2407.15673",
        "Date":2024,
        "Venue":[
            "arXiv"
        ],
        "Goal":[
            "The goal of this paper is to introduce IDA",
            "a novel no-code web UI automation tool designed to empower business users with no technical background",
            "and to validate its effectiveness and user experience through a user study with real-world business users."
        ],
        "Research Questions":[
            "1. How can a no-code web UI automation tool be built to empower business users with no technical background?\n2. Assess the capability of non-technical business users to successfully create automation with IDA.\n3. Collect user feedback on IDA\u2019s overall value and user experience",
            "measuring satisfaction and trust."
        ],
        "Key Findings":[
            "1. IDA",
            "a novel no-code web UI automation tool",
            "was designed to empower business users with no technical background to create automations using human-centric design principles.\n2. The study developed a prototype of IDA and conducted a user study with real-world business users",
            "which demonstrated that users could effectively utilize IDA to create automation.\n3. The qualitative feedback from the user study indicated that IDA is perceived as user-friendly and trustworthy by business users."
        ],
        "Future Work":[
            "1. Developing a natural language-based approach to allow users to outline their automation tasks at a high level before demonstrating them.\n2. Improving the algorithms that merge multiple user demonstrations into a cohesive automation",
            "to better understand user intent and semantic context.\n3. Adding a chat or dialog interface to IDA that supports multi-modal interactions",
            "to further enhance user engagement and the ability to refine automations.\n4. Expanding IDA's functionality beyond web UI automation to include desktop and mobile platforms",
            "which would require new approaches beyond the current DOM-based strategies",
            "such as leveraging computer vision and large multi-modal models."
        ],
        "Scope from our taxonomy":[
            "Form"
        ],
        "SDLC stage from waterfall":[
            "Unspecified"
        ],
        "Context":[
            "Professional"
        ],
        "Eligibility score":[

        ],
        "ID":"48"
    },
    {
        "Paper":"Identifying the Factors That Influence Trust in AI Code Completion",
        "Authors":[
            "Adam Brown",
            "Sarah D'Angelo",
            "Ambar Murillo",
            "Ciera Jaspan",
            "Collin Green"
        ],
        "Affiliation":[
            "Industry"
        ],
        "DOI":"https:\/\/doi.org\/10.1145\/3664646.3664757",
        "Date":2024,
        "Venue":[
            "AIware'24"
        ],
        "Goal":[
            "The goal of this paper is to identify the factors that influence developers' trust in AI-powered code completion tools",
            "in order to enable informed design decisions that support developers in building appropriate trust in these tools and potentially unlock large productivity benefits."
        ],
        "Research Questions":[
            "1) What factors influence developers' trust in AI-powered code completion?\n2) How can we measure the trustworthiness of AI-powered software development tools?"
        ],
        "Key Findings":[
            "1. Characteristics of the AI suggestion itself",
            "such as the quality and length of the suggestion",
            "influence developers' trust in the suggestion.\n2. Developers' expertise and familiarity with the code and the programming language influence their trust in the AI suggestions.\n3. The context of the development work",
            "such as whether the suggestion was in a test file",
            "influenced developers' trust in the AI suggestions.\n4. One of the strongest positive predictors of accepting a suggestion was the model quality score (predicted accuracy or model confidence) for that completion."
        ],
        "Future Work":[
            "1. Evaluating the impact of changes to AI models and developer tool designs on developer trust",
            "as measured by acceptance rates and developer sentiments.\n2. Investigating whether accepted AI-generated code persists through to being submitted",
            "rather than being heavily edited by the developer.\n3. Expanding the research to investigate developer trust in other AI-powered tools used in the development workflow",
            "such as chat interactions and test generation."
        ],
        "Scope from our taxonomy":[
            "Impact",
            "Quality"
        ],
        "SDLC stage from waterfall":[
            "Unspecified"
        ],
        "Context":[
            "Professional"
        ],
        "Eligibility score":[

        ],
        "ID":"49"
    },
    {
        "Paper":"Improving Steering and Verification in AI-Assisted Data Analysis with Interactive Task Decomposition",
        "Authors":[
            "Majeed Kazemitabaar",
            "Jack Williams",
            "Austin Z Henley",
            "Carina Negreanu"
        ],
        "Affiliation":[
            "Academia",
            "Industry"
        ],
        "DOI":"https:\/\/doi.org\/10.1145\/3654777.3676345",
        "Date":2024,
        "Venue":[
            "UIST'24"
        ],
        "Goal":[
            "The goal of this paper is to explore the design space of AI-assisted data analysis tools and present two novel interfaces",
            "Stepwise and Phasewise",
            "that focus on interactive task decomposition to improve steering and verification of the AI's outputs."
        ],
        "Research Questions":[
            "1) What are the challenges users face in verifying and steering AI-powered data analysis tools?\n2) Can interactive task decomposition approaches (Stepwise and Phasewise) help address these challenges compared to a conversational baseline?\n3) How do the Stepwise and Phasewise approaches differ in terms of user control",
            "intervention",
            "correction",
            "and verification compared to the baseline?"
        ],
        "Key Findings":[
            "1. Participants felt significantly more in control of the data analysis process when using the Phasewise and Stepwise systems compared to the Conversational baseline.\n2. Participants found the Phasewise and Stepwise systems significantly easier to intervene and fix whenever the AI was doing something wrong compared to the Conversational baseline.\n3. The Stepwise system's step-by-step task decomposition improved participants' confidence in verification compared to the Conversational baseline."
        ],
        "Future Work":[
            "1) Conducting longitudinal studies to validate the findings over longer-term use.\n2) Addressing limitations in how the system propagates user edits",
            "both upstream and downstream.\n3) Exploring ways to better propagate edits to assumptions and code to maintain consistency."
        ],
        "Scope from our taxonomy":[
            "Form",
            "Impact"
        ],
        "SDLC stage from waterfall":[
            "Unspecified"
        ],
        "Context":[
            "Professional"
        ],
        "Eligibility score":[

        ],
        "ID":"50"
    },
    {
        "Paper":"In-IDE Generation-based Information Support with a Large Language Model",
        "Authors":[
            "Daye Nam",
            "Andrew Macvean",
            "Vincent Hellendoorn",
            "Bogdan Vasilescu",
            "Brad Myers"
        ],
        "Affiliation":[
            "Academia",
            "Industry"
        ],
        "DOI":"https:\/\/doi.org\/10.1145\/3597503.3639187",
        "Date":2024,
        "Venue":[
            "ICSE'24"
        ],
        "Goal":[
            "The goal of this paper is to investigate the potential of large language models (LLMs) to assist developers in understanding code and learning new concepts and APIs",
            "and to explore how developers interact with and perceive the usefulness of an LLM-powered information support tool within an IDE."
        ],
        "Research Questions":[
            "RQ1: To what extent does GILT affect developers\u2019 understanding",
            "task completion time",
            "and task completion rates when faced with unfamiliar code?\nRQ2: How do developers interact with GILT",
            "and to what extent does that differ between the participants?\nRQ3: How do developers perceive the usefulness of GILT?"
        ],
        "Key Findings":[
            "1. There are statistically significant gains in task completion rate when using GILT",
            "compared to a web search",
            "but the degree of the benefit varies between students and professionals.\n2. Overall",
            "participants used Overview and Prompt-context most frequently. However",
            "the way participants interact with GILT varied based on their learning styles and familiarity with other AI tools.\n3. Participants appreciated GILT\u2019s ability to incorporate their code as context easily",
            "but some participants reported that writing a good prompt is still a challenge."
        ],
        "Future Work":[
            "1. Research on developers\u2019 motivations and reasons for code comprehension when LLMs are available will be valuable in informing future tool designs.\n2. Further research is needed",
            "exploring various interaction options to support a diverse developer population\n3.  To fully understand the implications of deploying this tool into general developer pipelines",
            "it is necessary to observe how programmers use it in real-world settings with larger-scale software systems",
            "less specific goals",
            "and over a longer time frame"
        ],
        "Scope from our taxonomy":[
            "Form",
            "Impact"
        ],
        "SDLC stage from waterfall":[
            "Unspecified"
        ],
        "Context":[
            "Professional"
        ],
        "Eligibility score":[

        ],
        "ID":"51"
    },
    {
        "Paper":"Investigating and Designing for Trust in AI-powered Code Generation Tools",
        "Authors":[
            "Ruotong Wang",
            "Ruijia Cheng",
            "Thomas Zimmermann"
        ],
        "Affiliation":[
            "Academia",
            "Industry"
        ],
        "DOI":"https:\/\/doi.org\/10.1145\/3630106.3658984",
        "Date":2024,
        "Venue":[
            "FAccT '24"
        ],
        "Goal":[
            "The goal of this paper is to investigate how software developers evaluate the trustworthiness of AI-powered code generation tools in real-world settings",
            "and to explore design concepts that can help developers make effective trust judgments."
        ],
        "Research Questions":[
            "1. What contributes to developers' trust attitudes in AI code generation tools? \n2. What are the challenges developers face in making trust judgments? \n3. What design affordances can support developers in evaluating the trustworthiness of AI-powered code generation tools?"
        ],
        "Key Findings":[
            "1. Developers tend to trust AI tools when they perceive practical benefits",
            "alignment with their\ngoals",
            "and trustworthy processes. Furthermore",
            "developers adjust their trust by considering additional situational factors such as task complexity and importance. \n2. Current AI tools do not provide enough support for the developers to assess AI tools\u2019 ability and benevolence in specific situations\n3. The researchers explored three sets of design concepts (suggestion quality indicators",
            "usage stats",
            "and control mechanisms) to help developers make effective trust judgments."
        ],
        "Future Work":[
            "1. Future systems can explore how to introduce the control options more clearly.\n2. Future research can build on our qualitative investigation by implementing and evaluating interactive prototypes in controlled experiments to better quantify the effects of interface design on users\u2019 trust.\n3. To gain a more in-depth understanding of how female and gender minority developers approach trust in AI tools\n4. To explore the effect on trust given the fast-growing adoptions in different communities and organizational settings"
        ],
        "Scope from our taxonomy":[
            "Form",
            "Impact"
        ],
        "SDLC stage from waterfall":[
            "Unspecified"
        ],
        "Context":[
            "Professional"
        ],
        "Eligibility score":[

        ],
        "ID":"52"
    },
    {
        "Paper":"Investigating Explainability of Generative AI for Code through Scenario-based Design",
        "Authors":[
            "Jiao Sun",
            "Q Vera Liao",
            "Michael Muller",
            "Mayank Agarwal",
            "Stephanie Houde",
            "Kartik Talamadupula",
            "Justin D Weisz"
        ],
        "Affiliation":[
            "Academia",
            "Industry"
        ],
        "DOI":"https:\/\/doi.org\/10.1145\/3490099.3511119",
        "Date":2022,
        "Venue":[
            "IUI'22"
        ],
        "Goal":[
            "The goal of the study is to explore users' explainability needs for generative AI (GenAI) models in the context of software engineering use cases",
            "including natural language to code",
            "code translation",
            "and code autocompletion."
        ],
        "Research Questions":[
            "1) What are the explainability needs of users for generative AI models in the context of software engineering tasks?\n2) How do these explainability needs differ from those for discriminative AI models used in decision-support systems?\n3) What types of XAI features can address the explainability needs for generative AI models in software engineering?"
        ],
        "Key Findings":[
            "1. The authors identified 11 categories of explainability needs for generative AI (GenAI) models for code generation",
            "including Input",
            "Output",
            "How (global)",
            "Performance",
            "Why\/Why Not",
            "Data",
            "System Requirements & Impact",
            "Limitations",
            "What if",
            "How to",
            "and Control.\n2. The authors proposed four types of XAI features to support users of GenAI for code: AI documentation",
            "indications of model uncertainty",
            "visualizations of model attention",
            "and social transparency",
            "and provided concrete design recommendations for these features."
        ],
        "Future Work":[
            "1. Exploring how explainability needs and solutions for GenAI can be contextualized by the characteristics of code artifacts and the software engineering domain.\n2. Evaluating the readiness and limitations of GenAI for code technologies",
            "and defining appropriate use cases that do not pose risks to stakeholders.\n3. Further investigating the design space for social transparency features to support users' understanding and effective use of GenAI across the software development lifecycle."
        ],
        "Scope from our taxonomy":[
            "Form"
        ],
        "SDLC stage from waterfall":[
            "Unspecified"
        ],
        "Context":[
            "Professional"
        ],
        "Eligibility score":[

        ],
        "ID":"53"
    },
    {
        "Paper":"Investigating Interaction Modes and User Agency in Human-LLM Collaboration for Domain-Specific Data Analysis",
        "Authors":[
            "Jiajing Guo",
            "Vikram Mohanty",
            "Jorge Ono",
            "Hongtao Hao",
            "Liang Gou",
            "Liu Ren"
        ],
        "Affiliation":[
            "Industry"
        ],
        "DOI":"https:\/\/doi.org\/10.1145\/3613905.3651042",
        "Date":2024,
        "Venue":[
            "CHI EA'24"
        ],
        "Goal":[
            "The goal of this paper is to explore the design of AI-powered domain-specific data analysis tools from the dimensions of interaction (open-ended vs. structured) and user agency (high vs. low)",
            "and to investigate how these design choices affect data scientists' perceptions and behaviors when using such tools."
        ],
        "Research Questions":[
            "RQ1: How do users perceive the outputs of LLMs in the context of domain-specific data analysis tasks?\nRQ2: What is the optimal design for LLM-powered tools to assist in data analysis tasks?"
        ],
        "Key Findings":[
            "1. Participants expressed overall satisfaction with the LLMs in assisting them with data analysis tasks",
            "while raising concerns about the need for transparency and ability to verify results.\n2. The different interaction modes (open-ended and structured workflow) and the two degrees of agency (high and low) resulted in different user behaviors",
            "each with its own set of advantages and limitations."
        ],
        "Future Work":[
            "1. Proposing new design ideas based on the design considerations and insights from the current study.\n2. Exploring different design options that cater to users with varying levels of skills and needs",
            "beyond just data scientists.\n3. Investigating how the tool could benefit users who have strong domain knowledge but limited programming skills."
        ],
        "Scope from our taxonomy":[
            "Form",
            "Impact"
        ],
        "SDLC stage from waterfall":[
            "Unspecified"
        ],
        "Context":[
            "Professional"
        ],
        "Eligibility score":[

        ],
        "ID":"54"
    },
    {
        "Paper":"Is GitHub Copilot a Substitute for Human Pair-Programming? An Empirical Study",
        "Authors":[
            "Saki Imai"
        ],
        "Affiliation":[
            "Academia"
        ],
        "DOI":"https:\/\/doi.org\/10.1145\/3510454.3522684",
        "Date":2022,
        "Venue":[
            "ICSE'22"
        ],
        "Goal":[
            "The goal of this paper is to empirically study the effectiveness of pair programming with GitHub Copilot in comparison to human pair-programming",
            "focusing on code productivity and code quality."
        ],
        "Research Questions":[
            "RQ1: Is there an advantage in productivity while using GitHub Copilot as compared to a human pair programmer?\nRQ2: What is the quality of code written with Copilot in comparison to human pair programmers?"
        ],
        "Key Findings":[
            "1. Pair programming with GitHub Copilot resulted in higher productivity",
            "as measured by the maximum and mean number of lines of code added",
            "compared to human pair programming.\n2. The code generated with Copilot had lower quality compared to human pair programming",
            "as measured by the higher number of lines of code deleted in the subsequent trial."
        ],
        "Future Work":[
            "1. To analyze the eye-tracking data collected during the experiment to compare how programmers inspect code generated by Copilot versus human pair-programming"
        ],
        "Scope from our taxonomy":[
            "Impact"
        ],
        "SDLC stage from waterfall":[
            "Unspecified"
        ],
        "Context":[
            "Professional"
        ],
        "Eligibility score":[

        ],
        "ID":"55"
    },
    {
        "Paper":"Is GitHub's Copilot as Bad as Humans at Introducing Vulnerabilities in Code?",
        "Authors":[
            "Owura Asare",
            "Meiyappan Nagappan",
            "N Asokan"
        ],
        "Affiliation":[
            "Academia"
        ],
        "DOI":"https:\/\/doi.org\/10.1007\/s10664-023-10380-1",
        "Date":2023,
        "Venue":[
            "Empirical Software Engineering Journal"
        ],
        "Goal":[
            "The goal of this publication is to conduct a comparative security evaluation of the AI code generation tool Copilot",
            "to determine if it is as likely to introduce vulnerabilities in code as human developers."
        ],
        "Research Questions":[
            "RQ: Is Copilot equally likely to generate the same vulnerabilities as human developers?"
        ],
        "Key Findings":[
            "Copilot is less likely to generate the same vulnerabilities as human developers",
            "implying that Copilot is not always as bad as human software developers."
        ],
        "Future Work":[
            "1. Investigating Copilot's behavior using its training data or more open language models to better understand code generation tools.\n2. Conducting a comparative user study to determine if the use of code generation tools like Copilot results in less secure code.\n3. Performing longitudinal studies on how code generation tools perform with respect to vulnerabilities of different ages."
        ],
        "Scope from our taxonomy":[
            "Quality"
        ],
        "SDLC stage from waterfall":[
            "Coding: In this phase the whole requirements will be converted to the production environment."
        ],
        "Context":[
            "Professional"
        ],
        "Eligibility score":[

        ],
        "ID":"56"
    },
    {
        "Paper":"Ivie: Lightweight Anchored Explanations of Just-Generated Code",
        "Authors":[
            "Litao Yan",
            "Alyssa Hwang",
            "Zhiyuan Wu",
            "Andrew Head"
        ],
        "Affiliation":[
            "Academia"
        ],
        "DOI":"https:\/\/doi.org\/10.1145\/3613904.3642239",
        "Date":2024,
        "Venue":[
            "CHI'24"
        ],
        "Goal":[
            "The goal of the study is to explore the notion of an \"instructive copilot\" - a programming assistant that not only generates code",
            "but also supports its understanding with timely",
            "anchored",
            "brief explanations."
        ],
        "Research Questions":[
            "1. Does Ivie improve understanding of generated code?\n2. Does Ivie influence how much attention programmers give to generated code? \n3. How distracting is Ivie?\n4. How does Ivie compare to chat-based AI code comprehension aids?"
        ],
        "Key Findings":[
            "1. Ivie improved programmers' understanding of generated code compared to a baseline chat-based AI assistant.\n2. Ivie reduced programmers' perceived task load compared to the baseline.\n3. Programmers preferred Ivie over the baseline and saw it as a complementary tool to other programming aids."
        ],
        "Future Work":[
            "1. Exploring the application of the instructive copilot concept to other domains beyond programming",
            "such as data analysis",
            "creative tools",
            "or other AI-generated content.\n2. Enhancing the design of the instructive copilot by incorporating additional design goals like expandability",
            "adaptability",
            "and configurability.\n3. Improving the accuracy and reliability of the AI-generated explanations",
            "perhaps by incorporating validation mechanisms or by leveraging multiple AI models."
        ],
        "Scope from our taxonomy":[
            "Form"
        ],
        "SDLC stage from waterfall":[
            "Unspecified"
        ],
        "Context":[
            "Professional"
        ],
        "Eligibility score":[

        ],
        "ID":"57"
    },
    {
        "Paper":"Lost at C: A User Study on the Security Implications of Large Language Model Code Assistants",
        "Authors":[
            "Gustavo Sandoval",
            "Hammond Pearce",
            "Teo Nys",
            "Ramesh Karri",
            "Siddharth Garg",
            "Brendan Dolan-Gavitt"
        ],
        "Affiliation":[
            "Academia"
        ],
        "DOI":"https:\/\/www.usenix.org\/conference\/usenixsecurity23\/presentation\/sandoval",
        "Date":2023,
        "Venue":[
            "USENIX Security'23"
        ],
        "Goal":[
            "The goal of this paper is to investigate the security implications of using large language model (LLM) code assistants",
            "such as OpenAI Codex",
            "on the code written by developers."
        ],
        "Research Questions":[
            "RQ1: Does an AI code assistant help novice users write better functional code? \nRQ2: Given functional benefits",
            "does the code that users write with AI assistance have an acceptable incidence rate of security bugs vis-a-vis code written without assistance? \nRQ3: How do AI-assisted users interact with potentially vulnerable code suggestions\u2014i.e.",
            "where do bugs originate in an LLM-assisted system?"
        ],
        "Key Findings":[
            "1. LLM code assistants like Codex improve developer productivity and functionality of the code written",
            "compared to the control group without LLM assistance.\n2. The security impact of using LLM code assistants is small",
            "with the AI-assisted group producing critical security bugs at a rate no greater than 10% higher than the control group.\n3. The origin of bugs in the AI-assisted group is split",
            "with 63% of bugs introduced manually by the users and 36% originating from the LLM suggestions."
        ],
        "Future Work":[
            "Not mentioned"
        ],
        "Scope from our taxonomy":[
            "Impact",
            "Quality"
        ],
        "SDLC stage from waterfall":[
            "Coding: In this phase the whole requirements will be converted to the production environment."
        ],
        "Context":[
            "Educational"
        ],
        "Eligibility score":[

        ],
        "ID":"58"
    },
    {
        "Paper":"Methodology for Code Synthesis Evaluation of LLMs Presented by a Case Study of ChatGPT and Copilot",
        "Authors":[
            "Zolt\u00e1n S\u00e1godi",
            "Istv\u00e1n Siket",
            "Rudolf Ferenc"
        ],
        "Affiliation":[
            "Academia",
            "Industry"
        ],
        "DOI":"doi: 10.1109\/ACCESS.2024.3403858",
        "Date":2024,
        "Venue":[
            "IEEE Access Journal"
        ],
        "Goal":[
            "The goal of this paper is to propose a methodology for evaluating and comparing the code synthesis capabilities of Large Language Models (LLMs) to help developers choose the best available model."
        ],
        "Research Questions":[
            "RQ 1: How does LLM-generated source code score in terms of source code quality?\nRQ 2: Is the generated source code accepted by developers?\nRQ 3: What aspects should be considered when choosing LLM-based generative tools?"
        ],
        "Key Findings":[
            "1. ChatGPT outperformed Copilot in terms of functional validity",
            "generating more perfect solutions on the program synthesis benchmark tasks.\n2. Both models generated good quality code in terms of technical validity",
            "but ChatGPT's code was more readable and modifiable according to the human evaluations.\n3. Overall",
            "the human evaluators preferred the code generated by ChatGPT over Copilot",
            "rating it higher in terms of first impression",
            "usability",
            "and acceptance."
        ],
        "Future Work":[
            "1) Developing a more automated framework for comparing LLMs in code generation",
            "including evaluating metrics like memory and time consumption.\n2) Using the comparison framework to identify the preferred LLMs among developers and investigate what features of those models are preferred",
            "in order to inform the development of even better models.\n3) Using the comparison methodology to study the effects of different prompting techniques and fine-tuning approaches",
            "in order to gain insights that can be used to further improve LLMs for code generation."
        ],
        "Scope from our taxonomy":[
            "Quality"
        ],
        "SDLC stage from waterfall":[
            "Unspecified"
        ],
        "Context":[
            "Professional"
        ],
        "Eligibility score":[

        ],
        "ID":"59"
    },
    {
        "Paper":"Multi-line AI-assisted Code Authoring",
        "Authors":[
            "Omer Dunay",
            "Daniel Cheng",
            "Adam Tait",
            "Parth Thakkar",
            "Peter C. Rigby",
            "Andy Chiu",
            "Imad Ahmad",
            "Arun Ganesan",
            "Chandra Maddila",
            "Vijayaraghavan Murali",
            "Ali Tayyebi",
            "Nachiappan Nagappan"
        ],
        "Affiliation":[
            "Industry"
        ],
        "DOI":"https:\/\/doi.org\/10.1145\/3663529.3663836",
        "Date":2024,
        "Venue":[
            "FSE'24"
        ],
        "Goal":[
            "The goal of the study is to describe how the authors scaled the CodeCompose product from displaying single-line suggestions to multi-line suggestions",
            "and the unique challenges they had to overcome to improve the usability of these multi-line suggestions for developers."
        ],
        "Research Questions":[
            "1. What is the impact of multi-line to 10\u2019s of thousands of developers."
        ],
        "Key Findings":[
            "1. Multi-line suggestions accounted for 42% of total characters accepted by users",
            "despite only accounting for 16% of displayed suggestions.\n2. Multi-line suggestions nearly doubled the percentage of keystrokes saved for users from 9% to 17%.\n3. Less than 1% of engineers opted out of using multi-line suggestions",
            "indicating widespread adoption and favorability."
        ],
        "Future Work":[
            "1. Exploring additional metrics beyond the ones currently tracked by the authors to further evaluate the effectiveness of multi-line suggestions.\n2. Conducting more research and deployment of code completion tools in large industrial environments",
            "especially with the rapid advancements in Generative AI.\n3. Further investigating the specific algorithms used for latency reduction and their impact in a production environment."
        ],
        "Scope from our taxonomy":[
            "Impact"
        ],
        "SDLC stage from waterfall":[
            "Coding: In this phase the whole requirements will be converted to the production environment."
        ],
        "Context":[
            "Professional"
        ],
        "Eligibility score":[

        ],
        "ID":"60"
    },
    {
        "Paper":"Non-Expert Programmers in the Generative AI Future",
        "Authors":[
            "Molly Q Feldman",
            "Carolyn Jane Anderson"
        ],
        "Affiliation":[
            "Academia"
        ],
        "DOI":"https:\/\/doi.org\/10.1145\/3663384.3663393",
        "Date":2024,
        "Venue":[
            "CHIWORK\u201924"
        ],
        "Goal":[
            "The goal of the paper is to investigate how non-expert programmers",
            "specifically Code Adaptors and Code Runners",
            "will be impacted by the advent of generative AI tools for programming (Code LLMs)",
            "establish a baseline for their effectiveness",
            "observe the effect of traditional CS1 education",
            "and propose key skills needed for non-experts to succeed in the era of generative AI."
        ],
        "Research Questions":[
            "1. How non-programmers work with Code LLMs to solve problems drawn from traditional Introduction to Computer Science (CS1) courses compared to beginners?"
        ],
        "Key Findings":[
            "1. Non-programmers struggled to effectively use the Code LLM",
            "with low success and prompt reliability rates compared to beginning programmers.\n2. Non-programmers struggled with technical communication",
            "including describing their intent in natural language and using the appropriate programming terminology.\n3. Non-programmers struggled to understand and evaluate the code generated by the Code LLM",
            "unlike beginning programmers who could at least identify specific issues with the generated code."
        ],
        "Future Work":[
            "1. Using open-source Code LLMs to better equalize access for researchers and users\n2. Investigating how non-experts interact with conversational Code LLMs that can explain code and engage in multi-turn interactions\n3. Exploring non-expert interactions with Code LLMs in more diverse populations and environments beyond the selective liberal arts college context of this study"
        ],
        "Scope from our taxonomy":[
            "Impact"
        ],
        "SDLC stage from waterfall":[
            "Unspecified"
        ],
        "Context":[
            "Educational"
        ],
        "Eligibility score":[

        ],
        "ID":"61"
    },
    {
        "Paper":"On the Concerns of Developers When Using GitHub Copilot",
        "Authors":[
            "Xiyu Zhou",
            "Peng Liang",
            "Beiqi Zhang",
            "Zengyang Li",
            "Aakash Ahmad",
            "Mojtaba Shahin",
            "Muhammad Waseem"
        ],
        "Affiliation":[
            "Academia"
        ],
        "DOI":"https:\/\/doi.org\/10.1016\/j.jss.2024.112204",
        "Date":2024,
        "Venue":[
            "Journal of Systems and Software"
        ],
        "Goal":[
            "The goal of this study is to identify the problems encountered by users when coding with Copilot",
            "as well as their underlying causes and potential solutions."
        ],
        "Research Questions":[
            "1. What are the problems faced by users while using Copilot in software development practice?\n2. What are the underlying causes of these problems?\n3. What are the potential solutions to address these problems?"
        ],
        "Key Findings":[
            "1. Operation Issue and Compatibility Issue are the most common problems faced by Copilot users.\n2. Copilot Internal Error",
            "Network Connection Error",
            "and Editor\/IDE Compatibility Issue are the most frequent causes of the problems.\n3. Bug Fixed by Copilot",
            "Modify Configuration\/Setting",
            "and Use Suitable Version are the predominant solutions to address the problems."
        ],
        "Future Work":[
            "1. Continuously collecting new data to analyze the latest usage problems",
            "causes",
            "and solutions for Copilot as it evolves.\n2. Conducting an industrial survey and interviews to gather more information about Copilot users' programming experience and development contexts",
            "and how these factors impact the problems they encounter.\n3. Comparing Copilot with other popular AI code assistant tools to gain valuable insights."
        ],
        "Scope from our taxonomy":[
            "Impact"
        ],
        "SDLC stage from waterfall":[
            "Unspecified"
        ],
        "Context":[
            "Professional"
        ],
        "Eligibility score":[

        ],
        "ID":"62"
    },
    {
        "Paper":"On the Design of AI-powered Code Assistants for Notebooks",
        "Authors":[
            "Andrew M Mcnutt",
            "Chenglong Wang",
            "Robert A Deline",
            "Steven M. Drucker"
        ],
        "Affiliation":[
            "Academia",
            "Industry"
        ],
        "DOI":"https:\/\/doi.org\/10.1145\/3544548.3580940",
        "Date":2023,
        "Venue":[
            "CHI'23"
        ],
        "Goal":[
            "The goal of this paper is to investigate the potential of code assistants in computational notebooks and identify challenges and opportunities for future systems in this space",
            "in order to empower future designers to build more helpful code assistants."
        ],
        "Research Questions":[
            "1. What choices are available in the design of AI-powered code assistants in notebooks?\n2. What do users expect from such assistants in this context?"
        ],
        "Key Findings":[
            "1. Participants strongly supported adapting Copilot-style assistants for notebooks",
            "favoring domain-specific or non-core task systems. While ambient interfaces provide low-friction recommendations",
            "their generality can lead to opaque use and lack of task specificity."
        ],
        "Future Work":[
            "1. Explore the design potential of nonlinear input systems",
            "such as code-generating spreadsheets (e.g.",
            "Mito).  \n2. Investigate bidirectionally synchronized projectional editors for handling complex interface tasks like visualization annotations.  \n3. Examine the feasibility and utility of code assistants that output only metadata",
            "such as code explanations."
        ],
        "Scope from our taxonomy":[
            "Form",
            "Impact"
        ],
        "SDLC stage from waterfall":[
            "Unspecified"
        ],
        "Context":[
            "Professional"
        ],
        "Eligibility score":[

        ],
        "ID":"63"
    },
    {
        "Paper":"On the Robustness of Code Generation Techniques: An Empirical Study on GitHub Copilot",
        "Authors":[
            "Antonio Mastropaolo",
            "Luca Pascarella",
            "Emanuela Guglielmi",
            "Matteo Ciniselli",
            "Simone Scalabrino",
            "Rocco Oliveto",
            "Gabriele Bavota"
        ],
        "Affiliation":[
            "Academia"
        ],
        "DOI":"DOI: 10.1109\/ICSE48619.2023.00181",
        "Date":2023,
        "Venue":[
            "ICSE'23"
        ],
        "Goal":[
            "The goal of this study is to understand how robust a state-of-the-art deep learning-based code completion approach",
            "specifically GitHub Copilot",
            "is to changes in the natural language description provided as input."
        ],
        "Research Questions":[
            "1. To what extent can automated paraphrasing techniques be used to test the robustness of DL-based code generators?\n2. To what extent is the output of GitHub Copilot influenced by the code description provided as input by the developer?"
        ],
        "Key Findings":[
            "1. State-of-the-art paraphrasing techniques can be used to test the robustness of deep learning-based code recommenders",
            "as they are able to generate semantically equivalent descriptions of a reference text in up to 77% of cases.\n2. Providing different but semantically equivalent natural language descriptions to the deep learning-based code recommender (GitHub Copilot) results in different code recommendations in \u223c46% of cases.\n3. The differences in the recommended code can result in the loss of correct recommendations",
            "with \u223c28% of test-passing methods only being obtained with either the original or the paraphrased descriptions."
        ],
        "Future Work":[
            "1. Running a controlled experiment with developers to assess the impact of different code descriptions on the recommendations received from DL-based code recommenders.\n2. Investigating how to customize the automatic paraphrasing techniques to further improve their performance on software-related text (such as methods' descriptions)."
        ],
        "Scope from our taxonomy":[
            "Quality"
        ],
        "SDLC stage from waterfall":[
            "Coding: In this phase the whole requirements will be converted to the production environment."
        ],
        "Context":[
            "Professional"
        ],
        "Eligibility score":[

        ],
        "ID":"64"
    },
    {
        "Paper":"Performance, Workload, Emotion, and Self-Efficacy of Novice Programmers Using AI Code Generation",
        "Authors":[
            "Nicholas Gardella",
            "Raymond Pettit",
            "Sara L Riggs"
        ],
        "Affiliation":[
            "Academia"
        ],
        "DOI":"https:\/\/doi.org\/10.1145\/3649217.3653615",
        "Date":2024,
        "Venue":[
            "ITiCSE 2024"
        ],
        "Goal":[
            "The goal of this paper is to investigate the effects of using an AI-driven Development Environment (AIDE) like GitHub Copilot on the performance",
            "workload",
            "emotion",
            "and self-efficacy of novice programmers under time pressure",
            "as well as how additional experience with the AIDE influences novices' performance and self-efficacy."
        ],
        "Research Questions":[
            "1. How do AIDEs affect the performance",
            "workload",
            "emotion",
            "and self-efficacy of novice programmers under time pressure?\n2. How do AIDEs influence the effects of additional time spent programming on novices' performance and self-efficacy."
        ],
        "Key Findings":[
            "1. Using the AI-driven development environment (AIDE) significantly improved programming performance",
            "reduced mental workload",
            "and reduced effort for novice programmers.\n2. With more experience using the AIDE",
            "novice programmers showed improvements in both their actual and perceived performance",
            "which did not occur when programming alone without the AIDE.\n3. AIDEs provide benefits that may tempt students to overuse them",
            "but they also have the potential to help struggling students build programming self-efficacy through new skills gained from using the technology."
        ],
        "Future Work":[
            "1. Extending the findings with broader groups of participants and comparing AI code generation to other programming assistance methods like internet search and pair programming.\n2. Evaluating the long-term effects of using AI code generation tools and focusing more on learning-specific outcomes.\n3. Investigating how much experience is needed for novice programmers to reach a plateau in their performance when using AI code generation tools.\n4. Debating the extent to which solo programming abilities should be expected of graduating students as AI code generation tools become more advanced and accessible."
        ],
        "Scope from our taxonomy":[
            "Impact"
        ],
        "SDLC stage from waterfall":[
            "Unspecified"
        ],
        "Context":[
            "Educational"
        ],
        "Eligibility score":[

        ],
        "ID":"65"
    },
    {
        "Paper":"Practices and Challenges of Using GitHub Copilot: An Empirical Study",
        "Authors":[
            "Beiqi Zhang",
            "Peng Liang",
            "Xiyu Zhou",
            "Aakash Ahmad",
            "Muhammad Waseem"
        ],
        "Affiliation":[
            "Academia"
        ],
        "DOI":"https:\/\/doi.org\/10.18293\/SEKE2023-077",
        "Date":2023,
        "Venue":[
            "SEKE"
        ],
        "Goal":[
            "The goal of the study is to understand the practices and challenges of using GitHub Copilot",
            "an AI-enabled tool for autocompleting source code",
            "based on data collected from Stack Overflow and GitHub Discussions."
        ],
        "Research Questions":[
            "RQ1: What programming languages are used with GitHub Copilot?\nRQ2: What IDEs are used with GitHub Copilot?\nRQ3: What technologies are used with GitHub Copilot?\nRQ4: What functions are implemented by using GitHub Copilot?\nRQ5: What are the benefits of using GitHub Copilot?\nRQ6: What are the limitations and challenges of using GitHub Copilot?"
        ],
        "Key Findings":[
            "1) The major programming languages used with Copilot are JavaScript and Python",
            "2) the main IDE used with Copilot is Visual Studio Code",
            "3) the most commonly used technology with Copilot is Node.js",
            "4) the leading function implemented by Copilot is data processing",
            "5) the significant benefit of using Copilot is useful code generation",
            "6) the main limitation encountered by practitioners when using Copilot is difficulty of integration."
        ],
        "Future Work":[
            "1. Further explore when to use Copilot",
            "for what specific purposes",
            "and by whom"
        ],
        "Scope from our taxonomy":[
            "Impact"
        ],
        "SDLC stage from waterfall":[
            "Coding: In this phase the whole requirements will be converted to the production environment."
        ],
        "Context":[
            "Professional"
        ],
        "Eligibility score":[

        ],
        "ID":"66"
    },
    {
        "Paper":"Practitioners' expectations on automated code comment generation",
        "Authors":[
            "Xing Hu",
            "Xin Xia",
            "David Lo",
            "Zhiyuan Wan",
            "Qiuyuan Chen",
            "Thomas Zimmermann"
        ],
        "Affiliation":[
            "Academia",
            "Industry"
        ],
        "DOI":"https:\/\/doi.org\/10.1145\/3510003.3510152",
        "Date":2022,
        "Venue":[
            "ICSE'22"
        ],
        "Goal":[
            "The goal of the study is to investigate practitioners' expectations on automated code comment generation tools."
        ],
        "Research Questions":[
            "1. What is the state of code commenting practices and what are the issues?\n2. Are automated code comment generation tools useful for practitioners?\n3. What are practitioners' expectations on code comment generation tools?\n4. How close are the current state-of-the-art studies to satisfy practitioner needs and demands before adoption?"
        ],
        "Key Findings":[
            "1. Practitioners often write code comments during software development",
            "but the quantity and quality of comments in their projects are limited. Many practitioners are confused when reading code without comments",
            "especially junior practitioners.\n2. The main commenting issues that practitioners face are lack of comments and generic comments that do not provide much information.\n3. 80% of the survey respondents think code comment generation tools have the potential to be useful for them",
            "as these tools can improve code readability",
            "development efficiency",
            "and help understand source code."
        ],
        "Future Work":[
            "1. Investigating evaluation criteria that practitioners value most",
            "such as the \"amount of additional information\" beyond what can be easily gleaned from the source code.\n2. Focusing on generating comments at the right locations instead of generating comments for all types of code units.\n3. Investigating techniques to generate comments that include information on how to use a method and why a method exists",
            "in addition to just describing the functionality."
        ],
        "Scope from our taxonomy":[
            "Impact"
        ],
        "SDLC stage from waterfall":[
            "Maintenance: After the software is already released",
            "it may need some modifications",
            "improvements",
            "errors correction",
            "and refinement accordingly."
        ],
        "Context":[
            "Professional"
        ],
        "Eligibility score":[

        ],
        "ID":"67"
    },
    {
        "Paper":"Practitioners' Expectations on Code Completion",
        "Authors":[
            "Chaozheng Wang",
            "Junhao Hu",
            "Cuiyun Gao",
            "Yu Jin",
            "Tao Xie",
            "Hailiang Huang",
            "Zhenyu Lei",
            "Yuetang Deng"
        ],
        "Affiliation":[
            "Academia",
            "Industry"
        ],
        "DOI":"https:\/\/doi.org\/10.1145\/3611643.3616280",
        "Date":2023,
        "Venue":[
            "ESEC\/FSE'23"
        ],
        "Goal":[
            "The goal of this paper is to investigate practitioners' expectations on code completion tools and compare them with the current research in this area",
            "in order to provide future research directions that can better meet the demands of practitioners."
        ],
        "Research Questions":[
            "RQ1: What is the state of code completion practices?\nRQ2: How important is code completion",
            "and what are the issues faced by practitioners when they use code completion?\nRQ3: What are practitioner expectations on code completion?\nRQ4: How well does existing research satisfy the practitioner's expectations?"
        ],
        "Key Findings":[
            "1. The most commonly used code completion tool is the built-in IDE tool; only about 13% of participants use third-party tools like IntelliCode and Copilot",
            "with 54% desiring better tools.\n2. Identifier completion and API recommendation are the most popular token-level code completion scenarios.\n3. API argument recommendation",
            "completion of the currently edited line",
            "and skeleton prediction are frequently used statement-level code completion scenarios.\n4. 88% of participants believe code completion tools are important",
            "with most agreeing these tools provide useful hints",
            "improve efficiency",
            "and enhance the programming experience.\n5. Major concerns include erroneous code completion",
            "difficult tool installation",
            "and unsatisfactory ranking of completions.\n6. Over 85% of participants agree that identifier completion",
            "API recommendation",
            "and path completion are essential for token-level code completion tools.\n7. Skeleton prediction",
            "line completion",
            "and API argument recommendation are the mo"
        ],
        "Future Work":[
            "Not mentioned"
        ],
        "Scope from our taxonomy":[
            "Impact"
        ],
        "SDLC stage from waterfall":[
            "Coding: In this phase the whole requirements will be converted to the production environment."
        ],
        "Context":[
            "Professional"
        ],
        "Eligibility score":[

        ],
        "ID":"68"
    },
    {
        "Paper":"Productivity Assessment of Neural Code Completion",
        "Authors":[
            "Albert Ziegler",
            "Eirini Kalliamvakou",
            "Shawn Simister",
            "Ganesh Sittampalam",
            "Alice Li",
            "Andrew Rice",
            "Devon Rifkin",
            "Edward Aftandilian",
            "Ikaliam Narphorium",
            "Hsenag Xalili",
            "Kalliamvakou Ziegler",
            "Sittampalam Simister"
        ],
        "Affiliation":[
            "Industry"
        ],
        "DOI":"https:\/\/doi.org\/10.1145\/3520312.3534864",
        "Date":2022,
        "Venue":[
            "MAPS'22"
        ],
        "Goal":[
            "The goal of this paper is to investigate whether usage measurements of developer interactions with GitHub Copilot can be used to predict perceived productivity as reported by developers",
            "and to provide a deeper dive into variations in acceptance rate over the developer population and over time."
        ],
        "Research Questions":[
            "What is the relationship between usage metrics of the GitHub Copilot code completion system and developers' perceived productivity?"
        ],
        "Key Findings":[
            "1. Acceptance rate of shown suggestions is a better predictor of perceived productivity than the alternative measures. However",
            "it is beneficial to combine with others to get a fuller picture.\n2. The acceptance rate varies significantly over developer population as well as over time"
        ],
        "Future Work":[
            "To further explore the analogy between code suggestions in an IDE and a conversation with a chatbot",
            "borrowing ideas from the evaluation of chatbots and natural language text generation."
        ],
        "Scope from our taxonomy":[
            "Impact"
        ],
        "SDLC stage from waterfall":[
            "Coding: In this phase the whole requirements will be converted to the production environment."
        ],
        "Context":[
            "Professional"
        ],
        "Eligibility score":[

        ],
        "ID":"69"
    },
    {
        "Paper":"Prompt Sapper: A LLM-Empowered Production Tool for Building AI Chains",
        "Authors":[
            "Yu Cheng",
            "Jieshan Chen",
            "Qing Huang",
            "Zhenchang Xing",
            "Xiwei Xu",
            "Qinghua Lu"
        ],
        "Affiliation":[
            "Academia",
            "Industry"
        ],
        "DOI":"https:\/\/doi.org\/10.1145\/3638247",
        "Date":2024,
        "Venue":[
            "ACM Transactions on Software Engineering and Methodology"
        ],
        "Goal":[
            "The goal of this paper is to systematize the AI chain methodology and develop a visual programming tool called Prompt Sapper that incorporates this methodology and LLM co-pilots to enable non-technical users to develop their own LLM-based AI chain services."
        ],
        "Research Questions":[
            "1. How to support the full life cycle of AI chain development?\n2. What is the usefulness of Prompt Sapper?\n3. What is the effectiveness of the two LLM-based co-pilots within Prompt Sapper?"
        ],
        "Key Findings":[
            "1. Authors develop an AI chain integrated development environment (IDE)",
            "Prompt Sapper with the aim of advancing the modularity",
            "composability",
            "debuggability",
            "and reusability of AI functionalities\n2. The results demonstrate the low entry barrier and practicality of our Prompt Sapper. It can significantly save time for AI chain engineers compared with using traditional IDEs for programming\nby alleviating the need to learn various API calls and reducing the time spent searching for online\nresource\n3. Co-pilots enhance the efficiency and effectiveness of the design phase of AI chain development",
            "further enhancing the overall utility of our system."
        ],
        "Future Work":[
            "1. To seamlessly integrate AI chain-based services into people\u2019s daily workflows",
            "providing customized work co-pilots"
        ],
        "Scope from our taxonomy":[
            "Form",
            "Impact"
        ],
        "SDLC stage from waterfall":[
            "Unspecified"
        ],
        "Context":[
            "Professional"
        ],
        "Eligibility score":[

        ],
        "ID":"70"
    },
    {
        "Paper":"Reading Between the Lines: Modeling User Behavior and Costs in AI-Assisted Programming",
        "Authors":[
            "Hussein Mozannar",
            "Gagan Bansal",
            "Adam Fourney",
            "Eric Horvitz"
        ],
        "Affiliation":[
            "Academia",
            "Industry"
        ],
        "DOI":"https:\/\/doi.org\/10.1145\/3613904.3641936",
        "Date":2024,
        "Venue":[
            "CHI'24"
        ],
        "Goal":[
            "The study aims to model and understand user behavior in AI-assisted programming environments",
            "specifically with GitHub Copilot",
            "to identify inefficiencies and support improvements in interface design and interaction metrics."
        ],
        "Research Questions":[
            "1. What activities do users undertake in anticipation for",
            "or to trigger a suggestion?\n2. What mental processes occur while the suggestions are onscreen",
            "and do people double-check suggestions before or after acceptance?\n3. How costly for users are these various new tasks",
            "and which take the most time?"
        ],
        "Key Findings":[
            "1. CUPS Taxonomy: A novel taxonomy (CodeRec User Programming States - CUPS) categorizes 12 programming states when interacting with Copilot",
            "including verifying suggestions",
            "prompt crafting",
            "and editing code.\n2. Time Allocation: Significant time (51.5%) is dedicated to Copilot-specific activities like suggestion verification and prompt crafting",
            "highlighting potential inefficiencies.\n3. Behavior Patterns: Frequent deferrals and post-hoc verifications suggest programmers often need additional time to confirm Copilot\u2019s suggestions."
        ],
        "Future Work":[
            "1. Predicting CUPS states: Develop real-time models to predict a programmer\u2019s CUPS state to optimize support during different activities.\n2. Assessing Individual Diferences: Explore interventions tailored to different user groups based on experience or usage patterns.\n3. Effect of Conditions and Tasks on Behavior: Study behavior changes with different Copilot versions and other AI code tools",
            "especially with varied tasks and programming languages\n4. Informing New Metrics: Perform a comparative study with and without Copilot to determine what the system adds or changes"
        ],
        "Scope from our taxonomy":[
            "Impact"
        ],
        "SDLC stage from waterfall":[
            "Unspecified"
        ],
        "Context":[
            "Professional"
        ],
        "Eligibility score":[

        ],
        "ID":"71"
    },
    {
        "Paper":"Significant Productivity Gains through Programming with Large Language Models",
        "Authors":[
            "Thomas Weber",
            "Maximilian Brandmaier",
            "Albrecht Schmidt",
            "Sven Mayer"
        ],
        "Affiliation":[
            "Academia"
        ],
        "DOI":"https:\/\/doi.org\/10.1145\/3661145",
        "Date":2024,
        "Venue":[
            "Proceedings of the ACM on Human-Computer Interaction Journal"
        ],
        "Goal":[
            "The goal of this paper is to explore how AI assistance for code generation impacts developer productivity",
            "using a holistic perspective that considers various factors contributing to overall productivity."
        ],
        "Research Questions":[
            "1. How do different forms of AI interaction (auto-complete vs. conversational) affect developer productivity and usage patterns?"
        ],
        "Key Findings":[
            "1. With either form of interaction",
            "AI assistants provide a strong benefit for developer productivity overall and many of the individual aspects of the SPACE framework\n2. The different forms of interaction (auto-complete vs. conversational) lead to distinct usage patterns and strategies by developers. Auto-completion tends to favor short snippets",
            "so presenting concise choices aligns with this usage pattern. In contrast",
            "the chatbot interface is preferred for longer prompts and responses",
            "providing ample space for detailed context.\n3. While the AI assistants improve productivity",
            "they do not significantly affect the objective quality of the produced code",
            "though participants tend to overestimate the quality of the AI-generated code."
        ],
        "Future Work":[
            "1. Future studies specifically targeting long-term users may yield further insights. \n2. It may also be interesting to see how development teams with multiple humans and AI assistants collaborate and how both expertise in AI and traditional development change the group dynamics."
        ],
        "Scope from our taxonomy":[
            "Form",
            "Impact",
            "Quality"
        ],
        "SDLC stage from waterfall":[
            "Unspecified"
        ],
        "Context":[
            "Professional"
        ],
        "Eligibility score":[

        ],
        "ID":"72"
    },
    {
        "Paper":"Slide4N: Creating Presentation Slides from Computational Notebooks with Human-AI Collaboration",
        "Authors":[
            "Fengjie Wang",
            "Xuye Liu",
            "Oujing Liu",
            "Ali Neshati",
            "Tengfei Ma",
            "Min Zhu",
            "Jian Zhao"
        ],
        "Affiliation":[
            "Academia",
            "Industry"
        ],
        "DOI":"https:\/\/doi.org\/10.1145\/3544548.3580753",
        "Date":2023,
        "Venue":[
            "CHI'23"
        ],
        "Goal":[
            "This study aims to explore the potential of human-AI collaboration to enhance the slide-creation process from computational notebooks. The paper presents and qualitatively assesses users' experience of Slide4N",
            "an interactive AI assistant designed to help data scientists efficiently generate slides from their notebooks."
        ],
        "Research Questions":[
            "design and develop Slide4N\nassess the usability of Slide4N in supporting presentation slides creation from computational notebooks.\nunderstand users\u2019 behaviors when creating presentation slides from computational notebooks with Slide4N. \ncollect users\u2019 attitudes and feedback on Slide4N\u2019s support for creating presentation slides from computing notebooks."
        ],
        "Key Findings":[
            "1. The development of an analytical pipeline that enables interactive and iterative slide creation from computational notebooks using NLP techniques.\n2. The creation of a human-AI collaborative tool called Slide4N that assists data scientists in creating presentation slides within the JupyterLab environment. \n3. An evaluation of Slide4N that demonstrates the value of the human-AI collaborative approach and provides design implications for similar tools."
        ],
        "Future Work":[
            "1. Incorporating information from markdown cells and code outputs to generate more comprehensive bullet points.\n2. Exploring automatic methods to generate alternative text descriptions for charts and tables on the slides to improve accessibility.\n3. Providing better support for structuring the slides",
            "such as allowing users to extract and reuse slide structures from previous presentations.\n4. Improving the styling options",
            "such as support for colors",
            "bolding",
            "and font size",
            "to better accommodate formal presentation needs."
        ],
        "Scope from our taxonomy":[
            "Form",
            "Impact"
        ],
        "SDLC stage from waterfall":[
            "Unspecified"
        ],
        "Context":[
            "Professional"
        ],
        "Eligibility score":[

        ],
        "ID":"73"
    },
    {
        "Paper":"Spellburst: A Node-based Interface for Exploratory Creative Coding with Natural Language Prompts",
        "Authors":[
            "Tyler Angert",
            "Miroslav Suzara",
            "Jenny Han",
            "Christopher Pondoc",
            "Hariharan Subramonyam"
        ],
        "Affiliation":[
            "Academia",
            "Industry"
        ],
        "DOI":"https:\/\/doi.org\/10.1145\/3586183.3606719",
        "Date":2023,
        "Venue":[
            "UIST'23"
        ],
        "Goal":[
            "The goal of the paper is to present a new creativity support tool that integrates natural language prompting and node-based programming to support exploratory creative coding and address the challenges that creative coders face in translating their semantic intents into syntactic code representations."
        ],
        "Research Questions":[
            "1. How can we incorporate generative AI capabilities within current creative programming workfows to support exploratory generative art-making?\n2. What are the challenges faced by creative coders in their exploratory workflows",
            "and how can these inform the design of a new creativity support tool?"
        ],
        "Key Findings":[
            "1. Design considerations for a creative coding system include maintaining fine-grained parameter control",
            "enabling easy manipulation and visibility of parameter values and outputs",
            "offering versioning and tracking of both visual outputs and code",
            "providing AI support tailored to different stages of the creative process",
            "and ensuring the artist retains creative control throughout.\n2. Participants engaged in rapid iterations",
            "both fine-grained and large changes",
            "encouraged by Spellburst's affordances",
            "found prompts effective for initiating larger creative shifts due to more expressive natural language",
            "and had mixed opinions on prompts for smaller syntactic variations",
            "with usefulness varying based on their clarity of intent.\n3. Evaluation with expert generative artists demonstrates that Spellburst enhances creative practice through rapid exploration while reducing the overhead of managing the exploration process."
        ],
        "Future Work":[
            "1. Improving the handling of errors and unexpected results",
            "especially when merging sketches.\n2. Enhancing the mapping between semantic prompts and the resulting syntactic changes in the code to make the system more predictable and less frustrating for users.\n3. Exploring multi-modal prompting that can build referential connections between the visual canvas and text-based prompts.\n4. Conducting larger and more diverse user studies",
            "as well as exploring a wider range of creative coding tasks and scenarios to better understand the system's capabilities and limitations."
        ],
        "Scope from our taxonomy":[
            "Form",
            "Impact"
        ],
        "SDLC stage from waterfall":[
            "Unspecified"
        ],
        "Context":[
            "Professional"
        ],
        "Eligibility score":[

        ],
        "ID":"74"
    },
    {
        "Paper":"Taking Flight with Copilot: Early insights and opportunities of AI-powered pair-programming tools",
        "Authors":[
            "Christian Bird",
            "Denae Ford",
            "Thomas Zimmermann",
            "Nicole Forsgren",
            "Eirini Kalliamvakou",
            "Travis Lowdermilk",
            "Idan Gazit"
        ],
        "Affiliation":[
            "Industry"
        ],
        "DOI":"https:\/\/doi.org\/10.1145\/3582083",
        "Date":2023,
        "Venue":[
            "ACM Queue"
        ],
        "Goal":[
            "The goal of this publication is to provide early insights and opportunities of AI-powered pair programming tools",
            "specifically focusing on the experiences of developers using GitHub's Copilot tool during its technical preview phase."
        ],
        "Research Questions":[
            "What are people using Copilot for? \nHow are first-timers engaging with Copilot?\nHow does Copilot impact productivity?"
        ],
        "Key Findings":[
            "Participants reported spending less time on Stack Overflow",
            "but now have less of an understanding of how or why the code works. \nParticipants accept the suggestion for efficiency but give up a small bit of autonomy\/control over the code they\u2019re writing. Observed participants wrestle with this in real time. \nCorrelation of 11 usage metrics to perceived productivity. The acceptance rate had the highest positive correlation with aggregate perceived productivity."
        ],
        "Future Work":[
            "Creating the right user experience such that the developer is helped more than hindered.\nFinding ways to help developers understand and assess code\u2014and the context within which that code executes and interacts. \nUnderstand the dynamics between developers and AI-powered tools.\nWhat factors are developers finding important to build trust? \nHow does trust get reconstructed after AI-powered developer tools perform in an unexpected manner? \nDoes AI-generated code lead to fewer (or more?) build breaks",
            "test failures",
            "or even post-release defects? \nShould AI-generated code have more or less scrutiny during code review? \nWhat proportion of shipping code comes from tools such as Copilot?"
        ],
        "Scope from our taxonomy":[
            "Impact",
            "Other"
        ],
        "SDLC stage from waterfall":[
            "Coding: In this phase the whole requirements will be converted to the production environment."
        ],
        "Context":[
            "Professional"
        ],
        "Eligibility score":[

        ],
        "ID":"75"
    },
    {
        "Paper":"The Impact of AI on Developer Productivity: Evidence from GitHub Copilot",
        "Authors":[
            "Sida Peng",
            "Eirini Kalliamvakou",
            "Peter Cihon",
            "Mert Demirer"
        ],
        "Affiliation":[
            "Industry"
        ],
        "DOI":"https:\/\/doi.org\/10.48550\/arXiv.2302.06590",
        "Date":2023,
        "Venue":[
            "arXiv"
        ],
        "Goal":[
            "The goal of the study is to measure the productivity impact of using the AI tool GitHub Copilot in programming tasks."
        ],
        "Research Questions":[
            "1. What is the productivity impact of using GitHub Copilot in programming tasks"
        ],
        "Key Findings":[
            "1) The treatment group using GitHub Copilot completed the task 55.8% faster than the control group.\n2) Developers with less programming experience",
            "older programmers",
            "and those who program more hours per day benefited the most from using GitHub Copilot.\n3) Participants underestimated the productivity gains from using GitHub Copilot",
            "with the actual 55.8% increase being higher than their 35% estimate."
        ],
        "Future Work":[
            "1. Exploring the productivity impacts of other AI tools or generative AI models in different professional contexts beyond software development.\n2. Further investigating the heterogeneous effects observed in the study",
            "such as the differential impacts on developers with varying levels of experience",
            "age",
            "and coding hours.\n3. Examining the potential of AI-powered tools like GitHub Copilot to expand access to software development careers",
            "particularly for less experienced or older developers."
        ],
        "Scope from our taxonomy":[
            "Impact"
        ],
        "SDLC stage from waterfall":[
            "Coding: In this phase the whole requirements will be converted to the production environment."
        ],
        "Context":[
            "Professional"
        ],
        "Eligibility score":[

        ],
        "ID":"76"
    },
    {
        "Paper":"The Programmer\u2019s Assistant: Conversational Interaction with a Large Language Model for Software Development",
        "Authors":[
            "Steven I Ross",
            "Fernando Martinez",
            "Stephanie Houde",
            "Michael Muller",
            "Justin D Weisz"
        ],
        "Affiliation":[
            "Industry"
        ],
        "DOI":"https:\/\/doi.org\/10.1145\/3581641.3584037",
        "Date":2023,
        "Venue":[
            "IUI'23"
        ],
        "Goal":[
            "The goal of the paper is to explore the utility and receptiveness of conversational interactions with a code-fluent large language model (LLM) for software development tasks",
            "and to provide empirical evidence of the value such a conversational programming assistant can provide."
        ],
        "Research Questions":[
            "1) What is the utility of a conversational interaction with a code-fluent large language model for software engineers?\n2) How receptive are software engineers to the idea of conversing with",
            "rather than just invoking",
            "a code-fluent AI assistant?"
        ],
        "Key Findings":[
            "1. Code-fluent LLMs can sufficiently support a conversational programming assistant.\n2. Despite an initial level of skepticism",
            "participants felt that a conversational assistant would provide value by improving their productivity.\n3. The additional contextual layers of conversational history and the artifact under development provide additional value to the co-creative process."
        ],
        "Future Work":[
            "1) Further developing human-centered AI systems that maximize human-AI collaboration\n2) Fine-tuning the language model for better conversational interaction\n3) Aligning the language model to be more helpful",
            "honest",
            "and harmless\n4) Integrating the language model with additional context from search-based approaches\n5) Improving the prompting mechanism",
            "including personalizing the initial prompt based on a user model"
        ],
        "Scope from our taxonomy":[
            "Form",
            "Impact"
        ],
        "SDLC stage from waterfall":[
            "Unspecified"
        ],
        "Context":[
            "Professional"
        ],
        "Eligibility score":[

        ],
        "ID":"77"
    },
    {
        "Paper":"The RealHumanEval: Evaluating Large Language Models' Abilities to Support Programmers",
        "Authors":[
            "Hussein Mozannar",
            "Valerie Chen",
            "Mohammed Alsobay",
            "Subhro Das",
            "Sebastian Zhao",
            "Dennis Wei",
            "Manish Nagireddy",
            "Prasanna Sattigeri",
            "Ameet Talwalkar",
            "David Sontag"
        ],
        "Affiliation":[
            "Academia",
            "Industry"
        ],
        "DOI":"https:\/\/doi.org\/10.48550\/arXiv.2404.02806",
        "Date":2024,
        "Venue":[
            "arXiv"
        ],
        "Goal":[
            "The goal of the paper is to evaluate large language models (LLMs) for code in a more human-centric way",
            "beyond just static benchmarks",
            "by introducing a web interface called RealHumanEval to measure the ability of LLMs to assist programmers through autocomplete or chat support",
            "and to investigate whether gains on existing benchmarks translate to gains in programmer productivity."
        ],
        "Research Questions":[
            "1) Whether improvements in LLM performance on static benchmarks translate to improvements in programmer productivity when using the LLMs.\n2) Whether human preference metrics",
            "such as code acceptance or copy rates",
            "are aligned with actual programmer productivity when using the LLMs."
        ],
        "Key Findings":[
            "1. Improvements in model performance on static benchmarks like HumanEval and MBPP lead to gains in human productivity",
            "particularly in the time spent completing tasks. This trend holds across both chat and autocomplete interactions.\n2. Human preference metrics like acceptance rate of suggestions and likelihood of copying code from chat responses do not correlate with actual programmer performance.\n3. The dissimilar findings between benchmarking and human preference metrics highlight the importance of careful evaluation to disentangle which metrics are indicative of downstream performance."
        ],
        "Future Work":[
            "1. Incorporating user feedback to improve LLM-based coding assistants\n2. Improving autocomplete and chat-based coding assistants through personalization",
            "dynamic suggestion length",
            "and better dialogue experiences\n3. Leveraging the user interaction data collected in the study to improve code LLMs"
        ],
        "Scope from our taxonomy":[
            "Form",
            "Impact",
            "Quality"
        ],
        "SDLC stage from waterfall":[
            "Unspecified"
        ],
        "Context":[
            "Professional"
        ],
        "Eligibility score":[

        ],
        "ID":"78"
    },
    {
        "Paper":"The Widening Gap: The Benefits and Harms of Generative AI for Novice Programmers",
        "Authors":[
            "James Prather",
            "Brent Reeves",
            "Stephen Macneil",
            "Arisoa S Randrianasolo",
            "Bailey Kimmel",
            "Jared Wright",
            "Ben Briggs",
            "Juho Leinonen",
            "Brett Becker"
        ],
        "Affiliation":[
            "Academia"
        ],
        "DOI":"https:\/\/doi.org\/10.1145\/3632620.3671116",
        "Date":2024,
        "Venue":[
            "ICER'24"
        ],
        "Goal":[
            "The goal of this paper is to explore the impact of generative AI (GenAI) tools on the metacognitive awareness and problem-solving abilities of novice programmers."
        ],
        "Research Questions":[
            "RQ1: What benefits do novice programmers receive from using GenAI tools to solve programming problems?\nRQ2: What difficulties do novice programmers face while using GenAI tools to solve programming problems?"
        ],
        "Key Findings":[
            "1. The introduction of generative AI (GenAI) tools like ChatGPT and GitHub Copilot has compounded the previously identified metacognitive difficulties that novice programmers face",
            "and has also introduced new metacognitive difficulties.\n2. Students with better grades and higher self-efficacy were more likely to use GenAI tools effectively",
            "while those with lower grades and self-efficacy struggled and developed new metacognitive difficulties.\n3.  Several of the participants in this study were able to accelerate to a solution thanks to use of GenAI tools\n4. The most common metacognitive difficulty identified was Location",
            "which GenAI often exacerbated",
            "creating an illusion of progress for many participants. Additionally",
            "discovered new metacognitive challenges associated with GenAI use: Progression",
            "Interruption",
            "and Mislead."
        ],
        "Future Work":[
            "1. Exploring the use of automated assessment tools to help novice programmers learn to distinguish between helpful and unhelpful GenAI suggestions.\n2. Replicating prior work on using code replays to improve novice metacognition",
            "but applying it in the context of GenAI tools.\n3. Explicitly teaching the programming problem solving process to novice programmers to help address the metacognitive issues identified with their use of GenAI tools."
        ],
        "Scope from our taxonomy":[
            "Impact"
        ],
        "SDLC stage from waterfall":[
            "Unspecified"
        ],
        "Context":[
            "Educational"
        ],
        "Eligibility score":[

        ],
        "ID":"79"
    },
    {
        "Paper":"Toward Effective AI Support for Developers: A survey of desires and concerns",
        "Authors":[
            "Mansi Khemka",
            "Brian Houck"
        ],
        "Affiliation":[
            "Industry"
        ],
        "DOI":"https:\/\/doi.org\/10.1145\/3690928",
        "Date":2024,
        "Venue":[
            "Communications of the ACM"
        ],
        "Goal":[
            "The goal of this publication is to understand the perspectives and concerns of software developers regarding the integration of AI into their workflows",
            "in order to guide the development and adoption of AI tools in a more informed and effective manner."
        ],
        "Research Questions":[
            "1. What aspects of their job would developers be most excited about AI helping with?\n2. What worries developers the most about integrating AI into their workflows?"
        ],
        "Key Findings":[
            "1. Developers are most excited about using AI to automate repetitive tasks such as writing tests and generating documentation.\n2. Developers are concerned that AI tools may not be able to handle the complexity and variety of real-world programming",
            "and that AI could introduce defects or vulnerabilities.\n3. Developers have some concerns about the potential impact of AI on job security",
            "but this is not a major concern for most."
        ],
        "Future Work":[
            "Not mentioned"
        ],
        "Scope from our taxonomy":[
            "Impact"
        ],
        "SDLC stage from waterfall":[
            "Unspecified"
        ],
        "Context":[
            "Professional"
        ],
        "Eligibility score":[

        ],
        "ID":"80"
    },
    {
        "Paper":"Towards Feature Engineering with Human and AI\u2019s Knowledge: Understanding Data Science Practitioners\u2019 Perceptions in Human&AI-Assisted Feature Engineering Design",
        "Authors":[
            "Qian Zhu",
            "Dakuo Wang",
            "April Yi Wang",
            "Udayan Khurana",
            "Xiaojuan Ma",
            "Shuai Ma",
            "Zixin Chen"
        ],
        "Affiliation":[
            "Academia"
        ],
        "DOI":"https:\/\/doi.org\/10.1145\/3643834.3661517",
        "Date":2024,
        "Venue":[
            "DIS'24"
        ],
        "Goal":[
            "The goal of this publication is to understand data science practitioners' perceptions and usage patterns when leveraging the combined knowledge of humans and AI for feature engineering (FE)."
        ],
        "Research Questions":[
            "1. How do DS practitioners inspect and select features that are generated using human&AI-assisted FE?\n2. What are DS practitioners' perceptions and attitudes toward the human&AI-assisted FE?\n3. What aspects of a human&AI-assisted FE recommendation system should be considered for improving its usability and user experience?"
        ],
        "Key Findings":[
            "1. Participants tended to choose features from both humans and AI",
            "keeping a balance between the two.\n2. Participants' reliance on the \"\"Creator\"\" (human or AI) of the features influenced their feature selection process",
            "with some strictly basing their selection on the Creator",
            "others using it as a reference",
            "and some disregarding it entirely.\n3. Participants perceived differences between human-generated and AI-generated features in terms of explainability",
            "complexity",
            "trustworthiness",
            "and scale."
        ],
        "Future Work":[
            "1. Conducting additional empirical evaluations and user studies to further understand how data science practitioners interact with and perceive the human-AI collaborative feature engineering system.\n2. Exploring the impact of the quantity of AI-recommended and human-recommended features on users' perceptions and adoption of the recommendations.\n3. Testing the human-AI feature engineering system with a wider range of stakeholders and applying it to various feature engineering tasks.\n4. Conducting a long-term study to observe how data science practitioners' behavior patterns and perceptions of the human-AI feature recommendations may change over time as they use the system."
        ],
        "Scope from our taxonomy":[
            "Impact"
        ],
        "SDLC stage from waterfall":[
            "Unspecified"
        ],
        "Context":[
            "Professional"
        ],
        "Eligibility score":[

        ],
        "ID":"81"
    },
    {
        "Paper":"Towards More Effective AI-Assisted Programming: A Systematic Design Exploration to Improve Visual Studio IntelliCode\u2019s User Experience",
        "Authors":[
            "Priyan Vaithilingam",
            "Elena L Glassman",
            "Peter Groenwegen",
            "Sumit Gulwani",
            "Austin Z Henley",
            "Rohan Malpani",
            "David Pugh",
            "Arjun Radhakrishna",
            "Gustavo Soares",
            "Joey Wang",
            "Aaron Yim"
        ],
        "Affiliation":[
            "Academia",
            "Industry"
        ],
        "DOI":"DOI: 10.1109\/ICSE-SEIP58684.2023.00022",
        "Date":2023,
        "Venue":[
            "ICSE-SEIP'23"
        ],
        "Goal":[
            "The goal of this paper is to explore and design effective inline interfaces for AI-assisted code change suggestions in order to improve their discoverability and usage."
        ],
        "Research Questions":[
            "How can we leverage inline interfaces to effectively show AI-assisted code changes?\nRQ1: Do participants notice the suggestions?\nRQ2: Do participants understand the proposed suggestions",
            "i.e.",
            "which code the system suggests removing and what code it suggests adding?\nRQ3: How much effort does it take to evaluate and act on the suggested changes?"
        ],
        "Key Findings":[
            "1. Inline code suggestions are more discoverable than suggestions presented in a lightbulb menu.\n2. Design Principles: Glanceable Suggestions",
            "Juxtaposition of Original and Suggested Code",
            "Simplicity Through Familiarity",
            "Sufficient Visibility for Validation",
            "Snoozability of Suggestions.\n3. The field deployment of the inline code suggestion interfaces led to a significant increase in usage",
            "with a 3.5x increase in regular users of the feature."
        ],
        "Future Work":[
            "1. Broaden studies to evaluate design principles in other IDEs.\n2. Explore adaptations of inline suggestions for complex",
            "multi-line changes.\n3. Refine snooze and frequency-control features to limit user distractions\u200b"
        ],
        "Scope from our taxonomy":[
            "Form"
        ],
        "SDLC stage from waterfall":[
            "Coding: In this phase the whole requirements will be converted to the production environment."
        ],
        "Context":[
            "Professional"
        ],
        "Eligibility score":[

        ],
        "ID":"82"
    },
    {
        "Paper":"Transforming Software Development: Evaluating the Efficiency and Challenges of GitHub Copilot in Real-World Projects",
        "Authors":[
            "Ruchika Pandey",
            "Prabhat Singh",
            "Raymond Wei",
            "Shaila Shankar"
        ],
        "Affiliation":[
            "Industry"
        ],
        "DOI":"https:\/\/doi.org\/10.48550\/arXiv.2406.17910",
        "Date":2024,
        "Venue":[
            "arXiv"
        ],
        "Goal":[
            "This study evaluates the efficiency gains",
            "areas for improvement",
            "and emerging challenges of using GitHub Copilot",
            "an AI-powered coding assistant."
        ],
        "Research Questions":[
            "1. What are the time savings and productivity improvements achieved by using GitHub Copilot across various software development tasks?\n2. What are specific scenarios and types of tasks where GitHub Copilot currently struggles or underperforms?"
        ],
        "Key Findings":[
            "1. Based on models created to approximate the percentage of time spent on different types of tasks in adding a feature in a SaaS product like that used in this study",
            "an efficiency gain of between 26% and 35% could be realized by using GitHub Copilot.\n2. GitHub Copilot has limited utility for advanced proprietary code",
            "such as code that implements unique business logic",
            "or where the relevant code is distributed over many files."
        ],
        "Future Work":[
            "1. Fine-tuning the internal codebases could potentially enhance the relevance of the generated code.\n2. For future studies",
            "additional tools and approaches can be studied",
            "including Agentic workflows",
            "to identify additional efficiencies that could be achieved to reduce cycle time by using Generative AI-based tools."
        ],
        "Scope from our taxonomy":[
            "Impact"
        ],
        "SDLC stage from waterfall":[
            "Unspecified"
        ],
        "Context":[
            "Professional"
        ],
        "Eligibility score":[

        ],
        "ID":"83"
    },
    {
        "Paper":"Trust in Generative AI among students: An Exploratory Study",
        "Authors":[
            "Matin Amoozadeh",
            "David Daniels",
            "Daye Nam",
            "Aayush Kumar",
            "Stella Chen",
            "Michael Hilton",
            "Srinivasa Sruti",
            "Ragavan",
            "Mohammad Amin Alipour",
            "Srinivasa Ragavan",
            "Mohammad Amin"
        ],
        "Affiliation":[
            "Academia"
        ],
        "DOI":"https:\/\/doi.org\/10.1145\/3626252.36308",
        "Date":2024,
        "Venue":[
            "SIGCSE'24"
        ],
        "Goal":[
            "The goal of this paper is to investigate students' trust in Generative AI (GenAI) and how it impacts their use and perception of these tools for programming tasks."
        ],
        "Research Questions":[
            "RQ1: How extensively do students use GenAI and what are their perceived benefits and drawbacks?\nRQ2: How much do students trust GenAI tools?\nRQ3: Overall",
            "how do students perceive GenAI in programming?"
        ],
        "Key Findings":[
            "1. The majority of students have used GenAI for both programming and non-programming tasks",
            "and there was no student that had not heard about GenAI.\n2. Students reported varying levels of trust in GenAI",
            "with ~16% expressing distrust",
            "36% remaining neutral",
            "and 47% reporting trust. There is a statistically significant correlation between trust in GenAI and self-reported improvements in student motivation and confidence in programming.\n3. About half of participants lacked trust in GenAI's ability and output",
            "emphasizing the need for human supervision",
            "while around a quarter found GenAI generally helpful",
            "and some believed it aids learning by enhancing concept understanding or clarifying question requirements."
        ],
        "Future Work":[
            "1. Future work must explore whether socio-economic or cultural factors underlie observed differences in beliefs about GenAI\u2019s capabilities.\n2. Further investigation is required to elucidate the exact nature of the causation between continuing-generation students' perceptions of confidence in programming and the extent to which GenAI helps them feel motivated and confident."
        ],
        "Scope from our taxonomy":[
            "Impact"
        ],
        "SDLC stage from waterfall":[
            "Unspecified"
        ],
        "Context":[
            "Educational"
        ],
        "Eligibility score":[

        ],
        "ID":"84"
    },
    {
        "Paper":"Using AI Assistants in Software Development: A Qualitative Study on Security Practices and Concerns",
        "Authors":[
            "Jan H Klemmer",
            "Stefan Albert Horstmann",
            "Nikhil Patnaik",
            "Cordelia Ludden",
            "Cordell Burton",
            "Carson Powers",
            "Fabio Massacci",
            "Akond Rahman",
            "Daniel Votipka",
            "Heather Richter",
            "Awais Rashid",
            "Alena Naiakshina",
            "Sascha Fahl"
        ],
        "Affiliation":[
            "Academia"
        ],
        "DOI":"https:\/\/doi.org\/10.1145\/3658644.3690283",
        "Date":2024,
        "Venue":[
            "ACM CCS 2024"
        ],
        "Goal":[
            "The goal of this paper is to investigate how software professionals use AI assistants in secure software development",
            "what security implications and considerations arise",
            "and what impact they foresee on secure software development."
        ],
        "Research Questions":[
            "RQ1: How are AI assistants used in software development in the context of security? \nRQ2: What security concerns and considerations are raised with AI assistants\u2019 usage in software development? \nRQ3: What do developers expect AI assistants\u2019 future impact on secure software development will be?"
        ],
        "Key Findings":[
            "1. AI assistants are widely used for various security-related tasks in software development",
            "such as threat modeling",
            "vulnerability detection",
            "and code generation",
            "in addition to general software development tasks.\n2. Despite security concerns",
            "participants continue to use AI assistants due to productivity gains",
            "though they remain skeptical and carefully validate the AI-generated outputs.\n3. Participants desire improvements in AI assistant quality",
            "correctness",
            "and security capabilities",
            "as well as more privacy-preserving solutions like self-hosted AI assistants."
        ],
        "Future Work":[
            "1. Study the use of not just AI code assistants",
            "but also general-purpose AI assistants in software development."
        ],
        "Scope from our taxonomy":[
            "Impact"
        ],
        "SDLC stage from waterfall":[
            "Unspecified"
        ],
        "Context":[
            "Professional"
        ],
        "Eligibility score":[

        ],
        "ID":"85"
    },
    {
        "Paper":"Using GitHub Copilot to Solve Simple Programming Problems",
        "Authors":[
            "Michel Wermelinger"
        ],
        "Affiliation":[
            "Academia"
        ],
        "DOI":"https:\/\/doi.org\/10.1145\/3545945.3569830",
        "Date":2023,
        "Venue":[
            "SIGCSE'23"
        ],
        "Goal":[
            "The goal of the study is to evaluate the performance of GitHub's Copilot AI assistant in solving simple programming problems",
            "generating tests",
            "and explaining code",
            "and to discuss the implications for teaching programming."
        ],
        "Research Questions":[
            "1. How does Copilot perform",
            "compared with Davinci",
            "in terms of the correctness and variety of the generated code",
            "tests and explanations?\n2. If a suggestion is incorrect",
            "can Copilot be interactively led to a correct one?"
        ],
        "Key Findings":[
            "1. Copilot performs less well than the Davinci model of OpenAI's Codex in terms of the correctness and variety of the generated code",
            "tests",
            "and explanations.\n2. While Copilot can provide a helpful first attempt at solving a problem",
            "students still need strong programming knowledge and skills to spot and fix Copilot's often incorrect suggestions.\n3. Copilot's code explanations are mostly low-level and lack important details",
            "so students still need to learn how to write clear",
            "high-level documentation and understand why code doesn't work in order to fix it."
        ],
        "Future Work":[
            "1. Studying how students actually use Copilot in practice",
            "to understand its real-world adoption and impact.\n2. Determining Copilot's capabilities and limitations",
            "in order to adapt teaching and assessment to the new reality of AI-powered programming assistants.\n3. Exploring new types of programming problems that go beyond simple",
            "well-defined tasks",
            "to better assess and develop students' coding",
            "debugging",
            "and algorithmic thinking skills in the presence of Copilot."
        ],
        "Scope from our taxonomy":[
            "Impact",
            "Quality"
        ],
        "SDLC stage from waterfall":[
            "Unspecified"
        ],
        "Context":[
            "Professional"
        ],
        "Eligibility score":[

        ],
        "ID":"86"
    },
    {
        "Paper":"Validating AI-Generated Code with Live Programming",
        "Authors":[
            "Kasra Ferdowsi",
            "Lisa Huang",
            "Michael B James",
            "Nadia Polikarpova",
            "Sorin Lerner"
        ],
        "Affiliation":[
            "Academia"
        ],
        "DOI":"https:\/\/doi.org\/10.1145\/3613904.3642495",
        "Date":2024,
        "Venue":[
            "CHI'24"
        ],
        "Goal":[
            "The goal of this paper is to investigate whether Live Programming (LP) can help address the challenge of validating AI-generated code."
        ],
        "Research Questions":[
            "1. How does Live Programming affect over-and under-reliance in validating AI-generated code?\n2. How does Live Programming affect validation strategies?\n3. How does Live Programming affect the cognitive load of validating AI-generated code?"
        ],
        "Key Findings":[
            "1. Live Programming (LP) can reduce over-reliance and under-reliance on AI-generated code by lowering the cost of validation through execution.\n2. Validation strategies depend on the task characteristics",
            "with participants spending more time examining code in algorithmic tasks and more time inspecting runtime values in API-heavy tasks.\n3. Live Programming lowers the cognitive load of validation by execution",
            "but not necessarily debugging",
            "where the cognitive load is dominated by the debugging process itself."
        ],
        "Future Work":[
            "1. Alleviate the burdens that LP places on the user",
            "such as providing complete programs",
            "generating test cases",
            "and highlighting relevant runtime information.\n2. Enable debugging and code repair of AI-generated code",
            "creating a tighter feedback loop where users can validate code in LP",
            "request repairs using the runtime information",
            "and then validate the repaired code in LP."
        ],
        "Scope from our taxonomy":[
            "Impact"
        ],
        "SDLC stage from waterfall":[
            "Unspecified"
        ],
        "Context":[
            "Professional"
        ],
        "Eligibility score":[

        ],
        "ID":"87"
    },
    {
        "Paper":"When to Show a Suggestion? Integrating Human Feedback in AI-Assisted Programming",
        "Authors":[
            "Hussein Mozannar",
            "Gagan Bansal",
            "Adam Fourney",
            "Eric Horvitz"
        ],
        "Affiliation":[
            "Academia",
            "Industry"
        ],
        "DOI":"https:\/\/doi.org\/10.1609\/aaai.v38i9.28878",
        "Date":2024,
        "Venue":[
            "AAAI'24 Technical Tracks"
        ],
        "Goal":[
            "The goal of this paper is to develop a method called Conditional Suggestion Display from Human Feedback (CDHF) that can selectively display code suggestions from AI-powered code recommendation systems like GitHub Copilot in order to improve programmer productivity."
        ],
        "Research Questions":[
            "1. When should the model inject a suggestion into the IDE?\n2. How well the CDHF procedure can make display decisions in a retrospective evaluation\n3. Which Suggestion to Show?"
        ],
        "Key Findings":[
            "1. The authors developed a utility-theoretic framework called Conditional Suggestion Display from Human Feedback (CDHF) to guide decisions about when to display code suggestions to programmers using AI-powered code recommendation systems.\n2. Using telemetry data from 535 programmers",
            "the authors showed that CDHF can hide 25% of suggestions that would have been rejected with 95% accuracy",
            "and avoid generating 13% of suggestions.\n3. The authors found that incorporating the programmer's latent unobserved state is important for accurately predicting suggestion acceptance.\n4. The authors showed that using suggestion acceptance as a reward signal to guide which suggestions to display can lead to lower quality suggestions",
            "indicating an unexpected pitfall."
        ],
        "Future Work":[
            "1. A user study is required to verify whether the method makes programmers more productive."
        ],
        "Scope from our taxonomy":[
            "Quality"
        ],
        "SDLC stage from waterfall":[
            "Coding: In this phase the whole requirements will be converted to the production environment."
        ],
        "Context":[
            "Professional"
        ],
        "Eligibility score":[

        ],
        "ID":"88"
    }
]